{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-24T17:59:10.961342Z",
     "start_time": "2024-03-24T17:59:06.102039Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import device, cuda, autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import flyvision\n",
    "from flyvision_ans import DECODING_CELLS\n",
    "from flyvision.utils.activity_utils import LayerActivity\n",
    "from from_retina_to_connectome_funcs import from_retina_to_model, get_decision_making_neurons, get_cell_type_indices, get_tensor_items, compute_accuracy\n",
    "from graph_models import GNNModel\n",
    "from from_video_to_training_batched_funcs import get_files_from_directory, select_random_videos, paths_to_labels, \\\n",
    "    load_custom_sequences\n",
    "\n",
    "device_type = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "DEVICE = device(device_type)\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2\n",
    "last_good_frame = 2\n",
    "\n",
    "TRAINING_DATA_DIR = \"easy_videos\"\n",
    "TESTING_DATA_DIR = \"easyval_videos\"\n",
    "\n",
    "debugging = True\n",
    "wandb_ = True\n",
    "\n",
    "NUM_CONNECTOME_PASSES=10"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# init stuff\n",
    "extent, kernel_size = 15, 13\n",
    "decision_making_vector = get_decision_making_neurons()\n",
    "receptors = flyvision.rendering.BoxEye(extent=extent, kernel_size=kernel_size)\n",
    "network_view = flyvision.NetworkView(flyvision.results_dir / \"opticflow/000/0000\")\n",
    "network = network_view.init_network(chkpt=\"best_chkpt\")\n",
    "classification = pd.read_csv(\"adult_data/classification_clean.csv\")\n",
    "root_id_to_index = pd.read_csv(\"adult_data/root_id_to_index.csv\")\n",
    "\n",
    "all_videos = get_files_from_directory(TRAINING_DATA_DIR)\n",
    "all_validation_videos = get_files_from_directory(TESTING_DATA_DIR)\n",
    "dt = 1 / 100 # some random parameter from flyvision\n",
    "\n",
    "cell_type_indices = get_cell_type_indices(classification, root_id_to_index, DECODING_CELLS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T17:59:16.323890Z",
     "start_time": "2024-03-24T17:59:10.965091Z"
    }
   },
   "id": "63b101e44d143f78",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = GNNModel(\n",
    "    num_node_features=1, \n",
    "    decision_making_vector=decision_making_vector,\n",
    "    num_passes=NUM_CONNECTOME_PASSES,\n",
    "    cell_type_indices=cell_type_indices,\n",
    "    batch_size=batch_size,\n",
    "    visual_input_persistence_rate=.8\n",
    ").to(DEVICE)\n",
    "lr = .01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = BCEWithLogitsLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T17:59:16.338387Z",
     "start_time": "2024-03-24T17:59:16.325147Z"
    }
   },
   "id": "4fce80a3502cf3c7",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33meudald\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.4"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/eudald/Desktop/doctorat/connectome/wandb/run-20240324_185917-dxhul0ot</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/eudald/adult_connectome/runs/dxhul0ot' target=\"_blank\">lilac-wind-77</a></strong> to <a href='https://wandb.ai/eudald/adult_connectome' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/eudald/adult_connectome' target=\"_blank\">https://wandb.ai/eudald/adult_connectome</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/eudald/adult_connectome/runs/dxhul0ot' target=\"_blank\">https://wandb.ai/eudald/adult_connectome/runs/dxhul0ot</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 57/8400 [09:44<23:46:50, 10.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m pred, label \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(predictions, true_labels):\n\u001B[1;32m     60\u001B[0m         data_table\u001B[38;5;241m.\u001B[39madd_data(pred, label)\n\u001B[0;32m---> 62\u001B[0m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m scaler\u001B[38;5;241m.\u001B[39mstep(optimizer)\n\u001B[1;32m     64\u001B[0m scaler\u001B[38;5;241m.\u001B[39mupdate()\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if wandb_:\n",
    "    wandb.init(\n",
    "        project=\"adult_connectome\", \n",
    "        config={\"learning_rate\": lr, \"batch_size\": batch_size}\n",
    "    )\n",
    "    data_table = wandb.Table(columns=[\"Predictions\", \"True Labels\"])\n",
    "\n",
    "probabilities = []\n",
    "accuracies = []\n",
    "already_selected = []\n",
    "for i in tqdm(range(len(all_videos) // batch_size)):\n",
    "    batch_files, already_selected = select_random_videos(\n",
    "        all_videos, batch_size, already_selected\n",
    "    )\n",
    "    labels = paths_to_labels(batch_files)\n",
    "    batch_sequences = load_custom_sequences(batch_files)\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "    \n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        # simulations are in RGB channels; move it to 0-1 for better training\n",
    "        simulation = torch.div(simulation, 256)\n",
    "        layer_activations.append(\n",
    "            LayerActivity(simulation, network.connectome, keepref=True)\n",
    "        )\n",
    "        \n",
    "    del rendered_sequences, simulation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    inputs, labels = from_retina_to_model(\n",
    "        layer_activations, labels, DECODING_CELLS, last_good_frame, classification, root_id_to_index\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    if wandb_:\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    labels = labels.to(DEVICE).unsqueeze(-1).float() \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(device_type):\n",
    "        out = model(inputs)\n",
    "        loss = criterion(out, labels)\n",
    "        # Convert logits to probabilities\n",
    "        prob = torch.sigmoid(out)\n",
    "        probabilities.append(prob)\n",
    "        accuracies.append(compute_accuracy(prob, labels))\n",
    "    \n",
    "    if wandb_:\n",
    "        wandb.log({\n",
    "            \"loss\": loss.item(), \n",
    "            \"acc\": sum(accuracies) / len(accuracies)}\n",
    "        )\n",
    "        \n",
    "        predictions = get_tensor_items(out)\n",
    "        true_labels = get_tensor_items(labels)\n",
    "        for pred, label in zip(predictions, true_labels):\n",
    "            data_table.add_data(pred, label)\n",
    "        \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if debugging and i == 99:\n",
    "        break\n",
    "\n",
    "if wandb_:\n",
    "    wandb.log({\"predictions_vs_labels\": data_table})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:09:03.822917Z",
     "start_time": "2024-03-24T17:59:16.339592Z"
    }
   },
   "id": "f8fa2f7661646a93",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "already_selected_validation = []\n",
    "# Assuming batch_size is defined\n",
    "for _ in tqdm(range(len(all_validation_videos) // batch_size)):\n",
    "    batch_files, already_selected_validation = select_random_videos(all_validation_videos, batch_size, already_selected_validation)\n",
    "\n",
    "    labels = paths_to_labels(batch_files)  # Convert paths to labels\n",
    "    batch_sequences = load_custom_sequences(batch_files)  # Load and preprocess the video sequences\n",
    "    \n",
    "    # Assuming receptors is a function that processes your sequences\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "    \n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(LayerActivity(simulation, network.connectome, keepref=True))\n",
    "        \n",
    "    del rendered_sequences, simulation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Preparing the data for the model, similar to training\n",
    "    inputs, labels = from_retina_to_model(layer_activations, labels, DECODING_CELLS, last_good_frame, classification, root_id_to_index)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).unsqueeze(-1).float()\n",
    "        \n",
    "        with autocast(device_type):\n",
    "            predictions = model(inputs)\n",
    "            # Assuming your criterion and evaluation metrics are defined similarly to training\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            # Calculate other metrics if necessary, e.g., accuracy\n",
    "            \n",
    "            # Log validation metrics to WandB\n",
    "            wandb.log({\"validation_loss\": loss.item()})\n",
    "            # Log other metrics similarly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33a081fe1b9a883e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e93414a83f7bbd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When cuda breaks:\n",
    "sudo rmmod nvidia_uvm\n",
    "sudo modprobe nvidia_uvm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dabc4d2827d8cc4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ba39b3687e323bec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
