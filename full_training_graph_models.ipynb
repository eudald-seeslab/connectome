{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:40:02.816615Z",
     "start_time": "2024-03-28T14:39:58.709963Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import device, cuda, autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import flyvision\n",
    "from flyvision_ans import DECODING_CELLS\n",
    "from flyvision.utils.activity_utils import LayerActivity\n",
    "from from_retina_to_connectome_funcs import from_retina_to_model, get_decision_making_neurons, get_cell_type_indices, compute_accuracy, get_tensor_items\n",
    "from logs_to_wandb import log_images_to_wandb\n",
    "from graph_models import GNNModel\n",
    "from from_video_to_training_batched_funcs import get_files_from_directory, select_random_videos, paths_to_labels, \\\n",
    "    load_custom_sequences\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='invalid value encountered in cast',\n",
    "    category=RuntimeWarning,\n",
    "    module='wandb.sdk.data_types.image'\n",
    ")\n",
    "\n",
    "device_type = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "DEVICE = device(device_type)\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2\n",
    "last_good_frame = 2\n",
    "\n",
    "TRAINING_DATA_DIR = \"videos/easy_videos\"\n",
    "TESTING_DATA_DIR = \"videos/easyval_videos\"\n",
    "\n",
    "debugging = True\n",
    "debug_length = 19\n",
    "wandb_ = True\n",
    "wandb_images_every = 5\n",
    "cell_type_plot = \"TmY18\"\n",
    "\n",
    "NUM_CONNECTOME_PASSES=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b101e44d143f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:18:59.226154Z",
     "start_time": "2024-03-28T14:18:54.651619Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init stuff\n",
    "extent, kernel_size = 15, 13\n",
    "decision_making_vector = get_decision_making_neurons()\n",
    "receptors = flyvision.rendering.BoxEye(extent=extent, kernel_size=kernel_size)\n",
    "network_view = flyvision.NetworkView(flyvision.results_dir / \"opticflow/000/0000\")\n",
    "network = network_view.init_network(chkpt=\"best_chkpt\")\n",
    "classification = pd.read_csv(\"adult_data/classification_clean.csv\")\n",
    "root_id_to_index = pd.read_csv(\"adult_data/root_id_to_index.csv\")\n",
    "\n",
    "all_videos = get_files_from_directory(TRAINING_DATA_DIR)\n",
    "all_validation_videos = get_files_from_directory(TESTING_DATA_DIR)\n",
    "dt = 1 / 100 # some random parameter from flyvision\n",
    "\n",
    "cell_type_indices = get_cell_type_indices(classification, root_id_to_index, DECODING_CELLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fce80a3502cf3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:18:59.238988Z",
     "start_time": "2024-03-28T14:18:59.227248Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GNNModel(\n",
    "    num_node_features=1, \n",
    "    decision_making_vector=decision_making_vector,\n",
    "    num_passes=NUM_CONNECTOME_PASSES,\n",
    "    cell_type_indices=cell_type_indices,\n",
    "    batch_size=batch_size,\n",
    "    visual_input_persistence_rate=.8\n",
    ").to(DEVICE)\n",
    "lr = .01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8fa2f7661646a93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:22:54.362080Z",
     "start_time": "2024-03-28T14:19:58.045941Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ad06ytyk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f50becce1749ad8973a3ba2685ab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-hill-88</strong> at: <a href='https://wandb.ai/eudald/adult_connectome/runs/ad06ytyk' target=\"_blank\">https://wandb.ai/eudald/adult_connectome/runs/ad06ytyk</a><br/> View job at <a href='https://wandb.ai/eudald/adult_connectome/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjA1MTM0Nw==/version_details/v29' target=\"_blank\">https://wandb.ai/eudald/adult_connectome/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjA1MTM0Nw==/version_details/v29</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240328_151900-ad06ytyk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ad06ytyk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc5380843574aadad731df1a6356405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112039977777183, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eudald/Desktop/doctorat/connectome/wandb/run-20240328_151958-51ys2ead</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eudald/adult_connectome/runs/51ys2ead' target=\"_blank\">lilac-fog-89</a></strong> to <a href='https://wandb.ai/eudald/adult_connectome' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eudald/adult_connectome' target=\"_blank\">https://wandb.ai/eudald/adult_connectome</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eudald/adult_connectome/runs/51ys2ead' target=\"_blank\">https://wandb.ai/eudald/adult_connectome/runs/51ys2ead</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:47<00:00,  8.84s/it]\n"
     ]
    }
   ],
   "source": [
    "if wandb_:\n",
    "    wandb.init(\n",
    "        project=\"adult_connectome\", \n",
    "        config={\"learning_rate\": lr, \"batch_size\": batch_size}\n",
    "    )\n",
    "    data_table = wandb.Table(columns=[\"Predictions\", \"True Labels\"])\n",
    "\n",
    "probabilities = []\n",
    "accuracies = []\n",
    "already_selected = []\n",
    "iterations = debug_length if debugging else len(all_videos) // batch_size\n",
    "\n",
    "for i in tqdm(range(iterations)):\n",
    "    batch_files, already_selected = select_random_videos(\n",
    "        all_videos, batch_size, already_selected\n",
    "    )\n",
    "    labels = paths_to_labels(batch_files)\n",
    "    batch_sequences = load_custom_sequences(batch_files)\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "    \n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        # rendered sequences are in RGB; move it to 0-1 for better training\n",
    "        rendered_sequence = torch.div(rendered_sequence, 255)\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(\n",
    "            LayerActivity(simulation, network.connectome, keepref=True)\n",
    "        )\n",
    "        \n",
    "    if wandb_ and i % wandb_images_every == 0:\n",
    "        log_images_to_wandb(batch_sequences[0], rendered_sequences[0], layer_activations[0], batch_files[0], frame=last_good_frame, cell_type=cell_type_plot)\n",
    "    \n",
    "    del rendered_sequences, rendered_sequence, simulation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    inputs, labels = from_retina_to_model(\n",
    "        layer_activations, labels, DECODING_CELLS, last_good_frame, classification, root_id_to_index\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    if wandb_:\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    labels = labels.to(DEVICE).unsqueeze(-1).float() \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(device_type):\n",
    "        out = model(inputs)\n",
    "        loss = criterion(out, labels)\n",
    "        # Convert logits to probabilities\n",
    "        prob = torch.sigmoid(out)\n",
    "        probabilities.append(prob)\n",
    "        accuracies.append(compute_accuracy(prob, labels))\n",
    "    \n",
    "    if wandb_:\n",
    "        wandb.log({\n",
    "            \"loss\": loss.item(), \n",
    "            \"acc\": sum(accuracies) / len(accuracies)}\n",
    "        )\n",
    "        \n",
    "        predictions = get_tensor_items(out)\n",
    "        true_labels = get_tensor_items(labels)\n",
    "        for pred, label in zip(predictions, true_labels):\n",
    "            data_table.add_data(pred, label)\n",
    "        \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "if wandb_:\n",
    "    wandb.log({\"predictions_vs_labels\": data_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33a081fe1b9a883e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T09:37:12.123162Z",
     "start_time": "2024-03-28T09:37:11.886793Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1960 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m batch_sequences \u001b[38;5;241m=\u001b[39m load_custom_sequences(batch_files)  \u001b[38;5;66;03m# Load and preprocess the video sequences\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming receptors is a function that processes your sequences\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m rendered_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mreceptors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m layer_activations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rendered_sequence \u001b[38;5;129;01min\u001b[39;00m rendered_sequences:\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/flyvision/rendering/eye.py:116\u001b[0m, in \u001b[0;36mBoxEye.__call__\u001b[0;34m(self, sequence, ftype, hex_sample)\u001b[0m\n\u001b[1;32m    111\u001b[0m samples, frames, height, width \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sequence, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mFloatTensor):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# auto-moving to GPU in case default tensor is cuda but passed\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# sequence is not for convenience\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_frame_size \u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([height, width]))\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# to rescale to the minimum frame size\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m ttf\u001b[38;5;241m.\u001b[39mresize(\n\u001b[1;32m    121\u001b[0m         sequence, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_frame_size\u001b[38;5;241m.\u001b[39mtolist(), antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "already_selected_validation = []\n",
    "# Assuming batch_size is defined\n",
    "for _ in tqdm(range(len(all_validation_videos) // batch_size)):\n",
    "    batch_files, already_selected_validation = select_random_videos(all_validation_videos, batch_size, already_selected_validation)\n",
    "\n",
    "    labels = paths_to_labels(batch_files)  # Convert paths to labels\n",
    "    batch_sequences = load_custom_sequences(batch_files)  # Load and preprocess the video sequences\n",
    "    \n",
    "    # Assuming receptors is a function that processes your sequences\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "    \n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(LayerActivity(simulation, network.connectome, keepref=True))\n",
    "        \n",
    "    del rendered_sequences, simulation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Preparing the data for the model, similar to training\n",
    "    inputs, labels = from_retina_to_model(layer_activations, labels, DECODING_CELLS, last_good_frame, classification, root_id_to_index)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).unsqueeze(-1).float()\n",
    "        \n",
    "        with autocast(device_type):\n",
    "            predictions = model(inputs)\n",
    "            # Assuming your criterion and evaluation metrics are defined similarly to training\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            # Calculate other metrics if necessary, e.g., accuracy\n",
    "            \n",
    "            # Log validation metrics to WandB\n",
    "            wandb.log({\"validation_loss\": loss.item()})\n",
    "            # Log other metrics similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93414a83f7bbd3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dabc4d2827d8cc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When cuda breaks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba39b3687e323bec",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for eudald: \n",
      "[sudo] password for eudald: "
     ]
    }
   ],
   "source": [
    "!sudo rmmod nvidia_uvm\n",
    "!sudo modprobe nvidia_uvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185e185",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
