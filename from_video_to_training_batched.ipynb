{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:36:38.973349Z",
     "start_time": "2024-03-06T10:36:35.096684Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import device, cuda, autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import flyvision\n",
    "from flyvision_ans import DECODING_CELLS\n",
    "from flyvision.utils.activity_utils import LayerActivity\n",
    "from from_retina_to_connectome_funcs import from_retina_to_model, get_decision_making_neurons\n",
    "from graph_models import GNNModel\n",
    "from from_video_to_training_batched_funcs import get_files_from_directory, select_random_videos, paths_to_labels, \\\n",
    "    load_custom_sequences\n",
    "\n",
    "device_type = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "DEVICE = device(device_type)\n",
    "torch.manual_seed(42)\n",
    "batch_size = 10\n",
    "last_good_frame = 2\n",
    "\n",
    "TRAINING_DATA_DIR = \"easy_videos\"\n",
    "TESTING_DATA_DIR = \"easyval_videos\""
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# init stuff\n",
    "extent, kernel_size = 15, 13\n",
    "decision_making_vector = get_decision_making_neurons()\n",
    "receptors = flyvision.rendering.BoxEye(extent=extent, kernel_size=kernel_size)\n",
    "network_view = flyvision.NetworkView(flyvision.results_dir / \"opticflow/000/0000\")\n",
    "network = network_view.init_network(chkpt=\"best_chkpt\")\n",
    "classification = pd.read_csv(\"adult_data/classification_clean.csv\")\n",
    "root_id_to_index = pd.read_csv(\"adult_data/root_id_to_index.csv\")\n",
    "\n",
    "all_videos = get_files_from_directory(TRAINING_DATA_DIR)\n",
    "all_validation_videos = get_files_from_directory(TESTING_DATA_DIR)\n",
    "dt = 1 / 100 # some random parameter from flyvision"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:36:43.348482Z",
     "start_time": "2024-03-06T10:36:38.975023Z"
    }
   },
   "id": "63b101e44d143f78",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = GNNModel(\n",
    "    num_node_features=1, \n",
    "    decision_making_vector=decision_making_vector\n",
    ").to(DEVICE)\n",
    "lr = .01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = BCEWithLogitsLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:36:43.353343Z",
     "start_time": "2024-03-06T10:36:43.349399Z"
    }
   },
   "id": "4fce80a3502cf3c7",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33meudald\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/eudald/Desktop/doctorat/connectome/wandb/run-20240306_113645-tsjymq5l</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/eudald/adult_connectome/runs/tsjymq5l' target=\"_blank\">peach-field-2</a></strong> to <a href='https://wandb.ai/eudald/adult_connectome' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/eudald/adult_connectome' target=\"_blank\">https://wandb.ai/eudald/adult_connectome</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/eudald/adult_connectome/runs/tsjymq5l' target=\"_blank\">https://wandb.ai/eudald/adult_connectome/runs/tsjymq5l</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 80/1680 [05:42<1:54:07,  4.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m layer_activations \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m rendered_sequence \u001B[38;5;129;01min\u001B[39;00m rendered_sequences:\n\u001B[0;32m---> 14\u001B[0m     simulation \u001B[38;5;241m=\u001B[39m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimulate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrendered_sequence\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     layer_activations\u001B[38;5;241m.\u001B[39mappend(LayerActivity(simulation, network\u001B[38;5;241m.\u001B[39mconnectome, keepref\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m rendered_sequences, simulation\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:498\u001B[0m, in \u001B[0;36mNetwork.simulate\u001B[0;34m(self, movie_input, dt, initial_state, as_states)\u001B[0m\n\u001B[1;32m    496\u001B[0m batch_size, n_frames \u001B[38;5;241m=\u001B[39m movie_input\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m initial_state \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 498\u001B[0m     initial_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msteady_state\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m simulation(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    500\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[1;32m    501\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m p\u001B[38;5;241m.\u001B[39mrequires_grad \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparameters()\n\u001B[1;32m    502\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:590\u001B[0m, in \u001B[0;36mNetwork.steady_state\u001B[0;34m(self, t_pre, dt, batch_size, value, state, grad, return_last)\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menable_grad(grad):\n\u001B[1;32m    589\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_last:\n\u001B[0;32m--> 590\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstimulus\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    591\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstimulus(), dt, as_states\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, state\u001B[38;5;241m=\u001B[39mstate)\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:545\u001B[0m, in \u001B[0;36mNetwork.forward\u001B[0;34m(self, x, dt, state, as_states)\u001B[0m\n\u001B[1;32m    542\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m state\n\u001B[1;32m    544\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m as_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 545\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    547\u001B[0m state_stack \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(\u001B[38;5;28mlist\u001B[39m(handle(state)), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    548\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m x, state\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:538\u001B[0m, in \u001B[0;36mNetwork.forward.<locals>.handle\u001B[0;34m(state)\u001B[0m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandle\u001B[39m(state):\n\u001B[1;32m    536\u001B[0m     \u001B[38;5;66;03m# loop over the temporal dimension for integration of dynamics\u001B[39;00m\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]):\n\u001B[0;32m--> 538\u001B[0m         state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    539\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m as_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    540\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m state\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mactivity\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:383\u001B[0m, in \u001B[0;36mNetwork._next_state\u001B[0;34m(self, params, state, x_t, dt)\u001B[0m\n\u001B[1;32m    368\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute the next state, given the current `state` and stimulus `x_t`.\u001B[39;00m\n\u001B[1;32m    369\u001B[0m \n\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;124;03mNote: simple, elementwise Euler integration.\u001B[39;00m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    381\u001B[0m vel \u001B[38;5;241m=\u001B[39m AutoDeref(nodes\u001B[38;5;241m=\u001B[39mAutoDeref(), edges\u001B[38;5;241m=\u001B[39mAutoDeref())\n\u001B[0;32m--> 383\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdynamics\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_state_velocity\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_sum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdt\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    387\u001B[0m next_state \u001B[38;5;241m=\u001B[39m AutoDeref(\n\u001B[1;32m    388\u001B[0m     nodes\u001B[38;5;241m=\u001B[39mAutoDeref(\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{k: state\u001B[38;5;241m.\u001B[39mnodes[k] \u001B[38;5;241m+\u001B[39m vel\u001B[38;5;241m.\u001B[39mnodes[k] \u001B[38;5;241m*\u001B[39m dt \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m state\u001B[38;5;241m.\u001B[39mnodes}\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    393\u001B[0m     ),\n\u001B[1;32m    394\u001B[0m )\n\u001B[1;32m    396\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_api(next_state)\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/dynamics.py:172\u001B[0m, in \u001B[0;36mPPNeuronIGRSynapses.write_state_velocity\u001B[0;34m(self, vel, state, params, target_sum, x_t, dt, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrite_state_velocity\u001B[39m(\u001B[38;5;28mself\u001B[39m, vel, state, params, target_sum, x_t, dt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    169\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Velocity is the bias plus the sum of the weighted rectified inputs.\"\"\"\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     vel\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mactivity \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    171\u001B[0m         \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 172\u001B[0m         \u001B[38;5;241m/\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(params\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mtime_const, \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[1;32m    173\u001B[0m         \u001B[38;5;241m*\u001B[39m (\n\u001B[1;32m    174\u001B[0m             \u001B[38;5;241m-\u001B[39mstate\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mactivity\n\u001B[1;32m    175\u001B[0m             \u001B[38;5;241m+\u001B[39m params\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mbias\n\u001B[1;32m    176\u001B[0m             \u001B[38;5;241m+\u001B[39m target_sum(\n\u001B[1;32m    177\u001B[0m                 params\u001B[38;5;241m.\u001B[39medges\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(state\u001B[38;5;241m.\u001B[39msources\u001B[38;5;241m.\u001B[39mactivity)\n\u001B[1;32m    178\u001B[0m             )  \u001B[38;5;66;03m# internal chemical current\u001B[39;00m\n\u001B[1;32m    179\u001B[0m             \u001B[38;5;241m+\u001B[39m x_t\n\u001B[1;32m    180\u001B[0m         )\n\u001B[1;32m    181\u001B[0m     )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"adult_connectome\", config={\"learning_rate\": lr, \"batch_size\": batch_size})\n",
    "\n",
    "already_selected = []\n",
    "for _ in tqdm(range(len(all_videos) // batch_size)):\n",
    "    batch_files, already_selected = select_random_videos(all_videos, batch_size, already_selected)\n",
    "    labels = paths_to_labels(batch_files)  # Assuming this function converts paths to labels\n",
    "    batch_sequences = load_custom_sequences(batch_files)\n",
    "    \n",
    "    # Assuming receptors is a function that processes your sequences\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "    \n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(LayerActivity(simulation, network.connectome, keepref=True))\n",
    "        \n",
    "    del rendered_sequences, simulation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Assuming from_retina_to_model prepares your data for the model, including labels\n",
    "    inputs, labels = from_retina_to_model(\n",
    "        layer_activations, labels, DECODING_CELLS, last_good_frame, classification, root_id_to_index\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    # Update the code to process the entire batch at once\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    labels = labels.to(DEVICE).unsqueeze(-1).float() \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(device_type):\n",
    "        out = model(inputs)\n",
    "        loss = criterion(out, labels)\n",
    "    \n",
    "    wandb.log({\"loss\": loss.item()})\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:42:28.643039Z",
     "start_time": "2024-03-06T10:36:43.354602Z"
    }
   },
   "id": "f8fa2f7661646a93",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 107/200 [06:50<07:25,  4.79s/it]"
     ]
    }
   ],
   "source": [
    "already_selected_validation = []\n",
    "# Assuming batch_size is defined\n",
    "for _ in tqdm(range(len(all_validation_videos) // batch_size)):\n",
    "    batch_files, already_selected_validation = select_random_videos(all_validation_videos, batch_size, already_selected_validation)\n",
    "\n",
    "    labels = paths_to_labels(batch_files)  # Convert paths to labels\n",
    "    batch_sequences = load_custom_sequences(batch_files)  # Load and preprocess the video sequences\n",
    "    \n",
    "    # Assuming receptors is a function that processes your sequences\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "    \n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(LayerActivity(simulation, network.connectome, keepref=True))\n",
    "        \n",
    "    del rendered_sequences, simulation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Preparing the data for the model, similar to training\n",
    "    inputs, labels = from_retina_to_model(layer_activations, labels, DECODING_CELLS, last_good_frame, classification, root_id_to_index)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).unsqueeze(-1).float()\n",
    "        \n",
    "        with autocast(device_type):\n",
    "            predictions = model(inputs)\n",
    "            # Assuming your criterion and evaluation metrics are defined similarly to training\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            # Calculate other metrics if necessary, e.g., accuracy\n",
    "            \n",
    "            # Log validation metrics to WandB\n",
    "            wandb.log({\"validation_loss\": loss.item()})\n",
    "            # Log other metrics similarly"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-06T09:56:11.124729Z"
    }
   },
   "id": "33a081fe1b9a883e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e93414a83f7bbd3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
