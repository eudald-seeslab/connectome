{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:29:20.549239Z",
     "start_time": "2024-04-02T15:29:20.542563Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import device, cuda, autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "from conv_models import AlternativeCNN\n",
    "from from_video_to_training_batched_funcs import get_files_from_directory, select_random_videos, paths_to_labels, load_custom_sequences\n",
    "from logs_to_wandb import log_original_to_wandb\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='invalid value encountered in cast',\n",
    "    category=RuntimeWarning,\n",
    "    module='wandb.sdk.data_types.image'\n",
    ")\n",
    "\n",
    "device_type = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "DEVICE = device(device_type)\n",
    "torch.manual_seed(42)\n",
    "batch_size = 50\n",
    "last_good_frame = 2\n",
    "\n",
    "TRAINING_DATA_DIR = \"easy_videos\"\n",
    "TESTING_DATA_DIR = \"easyval_videos\"\n",
    "\n",
    "debugging = False\n",
    "debug_length = 50\n",
    "wandb_ = True\n",
    "wandb_images_every = 10\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "channel_sizes = [16, 32, 64]\n",
    "mult_size = 1\n",
    "channel_sizes = [int(a * mult_size) for a in channel_sizes]\n",
    "dropout = .5"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_videos = get_files_from_directory(TRAINING_DATA_DIR)\n",
    "all_validation_videos = get_files_from_directory(TESTING_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:29:26.060057Z",
     "start_time": "2024-04-02T15:29:26.024095Z"
    }
   },
   "id": "63b101e44d143f78",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = AlternativeCNN(\n",
    "    out_channels_1 = channel_sizes[0],\n",
    "    out_channels_2 = channel_sizes[1],\n",
    "    out_channels_3 = channel_sizes[2],\n",
    "    dropout = dropout\n",
    ").to(DEVICE)\n",
    "lr = .01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = BCEWithLogitsLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:29:30.096137Z",
     "start_time": "2024-04-02T15:29:30.086623Z"
    }
   },
   "id": "4fce80a3502cf3c7",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:f36xuc19) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.753 MB of 0.753 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17ba3085c36e44ff8c3fd2e7611b69ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▂▁▂▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>███▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.9444</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.3375</td></tr><tr><td>val_accuracy</td><td>0.50308</td></tr><tr><td>val_loss</td><td>60.88941</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">resilient-water-20</strong> at: <a href='https://wandb.ai/eudald/binary_classification/runs/f36xuc19' target=\"_blank\">https://wandb.ai/eudald/binary_classification/runs/f36xuc19</a><br/> View job at <a href='https://wandb.ai/eudald/binary_classification/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1MzYwNjgwMQ==/version_details/v7' target=\"_blank\">https://wandb.ai/eudald/binary_classification/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1MzYwNjgwMQ==/version_details/v7</a><br/>Synced 5 W&B file(s), 36 media file(s), 4 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240402_165635-f36xuc19/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:f36xuc19). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112089811114048, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21018921aaa64748a873946921bfcb86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.4"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/eudald/Desktop/doctorat/connectome/wandb/run-20240402_172931-b4kkbwcd</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/eudald/binary_classification/runs/b4kkbwcd' target=\"_blank\">avid-dream-21</a></strong> to <a href='https://wandb.ai/eudald/binary_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/eudald/binary_classification' target=\"_blank\">https://wandb.ai/eudald/binary_classification</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/eudald/binary_classification/runs/b4kkbwcd' target=\"_blank\">https://wandb.ai/eudald/binary_classification/runs/b4kkbwcd</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [29:47<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.29656733177779687, Accuracy: 0.9551785714285714\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "iterations = debug_length if debugging else len(all_videos) // batch_size\n",
    "# Start wandb run\n",
    "if wandb_:\n",
    "    wandb.init(project='binary_classification')\n",
    "    data_table = wandb.Table(columns=[\"Predictions\", \"True Labels\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    already_selected = []\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in tqdm(range(iterations)):\n",
    "        batch_files, already_selected = select_random_videos(\n",
    "            all_videos, batch_size, already_selected\n",
    "        )\n",
    "        labels = paths_to_labels(batch_files)\n",
    "        batch_sequences = load_custom_sequences(batch_files)\n",
    "        batch_sequences = batch_sequences[:, -last_good_frame, :, :] \n",
    "        if wandb_ and i % wandb_images_every == 0:\n",
    "            log_original_to_wandb(batch_sequences[0], batch_files[0])\n",
    "        \n",
    "        # Add channel dimension and normalize\n",
    "        batch_sequences = np.divide(batch_sequences[:, None, :, :], 255)\n",
    "\n",
    "        # Ensure the data is in tensor form and on the correct device\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float).to(DEVICE)\n",
    "        batch_sequences_tensor = torch.tensor(batch_sequences, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "        # Create the dataset and dataloader\n",
    "        dataset = TensorDataset(batch_sequences_tensor, labels_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for inputs, batch_labels in dataloader:\n",
    "            inputs.to(DEVICE)\n",
    "            batch_labels.to(DEVICE)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            with autocast(device_type):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "    \n",
    "            # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                # Calculate the running loss and accuracy\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                predictions = torch.sigmoid(outputs).squeeze() > 0.5\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predictions == batch_labels).sum().item()\n",
    "                \n",
    "                if wandb_:\n",
    "                    predictions = torch.sigmoid(outputs).squeeze().detach().cpu()\n",
    "                    batch_labels_cpu = batch_labels.detach().cpu()\n",
    "                    for pred, label in zip(predictions, batch_labels_cpu):\n",
    "                        data_table.add_data(pred.item(), label.item())\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        if wandb_:\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'loss': running_loss / total,\n",
    "                'accuracy': correct / total,\n",
    "            })\n",
    "    wandb.log({\"predictions_vs_labels\": data_table})\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / total}, Accuracy: {correct / total}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T15:59:31.657634Z",
     "start_time": "2024-04-02T15:29:31.565122Z"
    }
   },
   "id": "a3b94263294e5262",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [04:48<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2448183917082273, Validation Accuracy: 0.9002564102564102\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "# Create a table for logging validation predictions and true labels to wandb\n",
    "if wandb_:\n",
    "    val_data_table = wandb.Table(columns=[\"Predictions\", \"True Labels\"])\n",
    "\n",
    "# No need to iterate over epochs in the validation phase\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for i in tqdm(range(len(all_validation_videos) // batch_size)):\n",
    "        batch_files, _ = select_random_videos(\n",
    "            all_validation_videos, batch_size, already_selected=[]\n",
    "        )\n",
    "        labels = paths_to_labels(batch_files)\n",
    "        batch_sequences = load_custom_sequences(batch_files)\n",
    "        batch_sequences = batch_sequences[:, -last_good_frame, :, :]\n",
    "        batch_sequences = np.divide(batch_sequences[:, None, :, :], 255)\n",
    "\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float).to(DEVICE)\n",
    "        batch_sequences_tensor = torch.tensor(batch_sequences, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "        dataset = TensorDataset(batch_sequences_tensor, labels_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for inputs, batch_labels in dataloader:\n",
    "            inputs, batch_labels = inputs.to(DEVICE), batch_labels.to(DEVICE)\n",
    "            \n",
    "            # Forward pass only to get logits/output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), batch_labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.sigmoid(outputs).squeeze() > 0.5\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predictions == batch_labels).sum().item()\n",
    "\n",
    "            if wandb_:\n",
    "                predictions = torch.sigmoid(outputs).squeeze().detach().cpu()\n",
    "                batch_labels_cpu = batch_labels.detach().cpu()\n",
    "                for pred, label in zip(predictions, batch_labels_cpu):\n",
    "                    val_data_table.add_data(pred.item(), label.item())\n",
    "\n",
    "# Log validation metrics to wandb\n",
    "if wandb_:\n",
    "    wandb.log({\n",
    "        'val_loss': running_loss / total,\n",
    "        'val_accuracy': correct / total,\n",
    "        \"val_predictions_vs_labels\": val_data_table\n",
    "    })\n",
    "\n",
    "print(f'Validation Loss: {running_loss / total}, Validation Accuracy: {correct / total}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T16:12:09.687306Z",
     "start_time": "2024-04-02T16:07:19.788296Z"
    }
   },
   "id": "e925cb0d6cfa7f80",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "19a61ca797bb8b81"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
