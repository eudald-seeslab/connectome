{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:44:08.930921Z",
     "start_time": "2024-03-23T16:44:01.884856Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import device, cuda, autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import flyvision\n",
    "from flyvision_ans import DECODING_CELLS\n",
    "from flyvision.utils.activity_utils import LayerActivity\n",
    "from from_retina_to_connectome_funcs import from_retina_to_model, get_decision_making_neurons, get_cell_type_indices\n",
    "from graph_models import GNNModel\n",
    "from from_video_to_training_batched_funcs import get_files_from_directory, select_random_videos, paths_to_labels, \\\n",
    "    load_custom_sequences\n",
    "\n",
    "device_type = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "DEVICE = device(device_type)\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2\n",
    "last_good_frame = 2\n",
    "\n",
    "TRAINING_DATA_DIR = \"easy_videos\"\n",
    "TESTING_DATA_DIR = \"easyval_videos\"\n",
    "\n",
    "debugging = True\n",
    "wandb_ = True\n",
    "\n",
    "NUM_CONNECTOME_PASSES=6"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# init stuff\n",
    "extent, kernel_size = 15, 13\n",
    "decision_making_vector = get_decision_making_neurons()\n",
    "receptors = flyvision.rendering.BoxEye(extent=extent, kernel_size=kernel_size)\n",
    "network_view = flyvision.NetworkView(flyvision.results_dir / \"opticflow/000/0000\")\n",
    "network = network_view.init_network(chkpt=\"best_chkpt\")\n",
    "classification = pd.read_csv(\"adult_data/classification_clean.csv\")\n",
    "root_id_to_index = pd.read_csv(\"adult_data/root_id_to_index.csv\")\n",
    "\n",
    "all_videos = get_files_from_directory(TRAINING_DATA_DIR)\n",
    "all_validation_videos = get_files_from_directory(TESTING_DATA_DIR)\n",
    "dt = 1 / 100 # some random parameter from flyvision\n",
    "\n",
    "cell_type_indices = get_cell_type_indices(classification, root_id_to_index, DECODING_CELLS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:44:15.186473Z",
     "start_time": "2024-03-23T16:44:10.103179Z"
    }
   },
   "id": "63b101e44d143f78",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = GNNModel(\n",
    "    num_node_features=1, \n",
    "    decision_making_vector=decision_making_vector,\n",
    "    num_passes=NUM_CONNECTOME_PASSES,\n",
    "    cell_type_indices=cell_type_indices,\n",
    "    batch_size=batch_size,\n",
    "    visual_input_persistence_rate=.8\n",
    ").to(DEVICE)\n",
    "lr = .01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = BCEWithLogitsLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:44:15.196804Z",
     "start_time": "2024-03-23T16:44:15.187840Z"
    }
   },
   "id": "4fce80a3502cf3c7",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33meudald\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112170855555487, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5bc90a174634921a94403b6ac4f9a74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/eudald/Desktop/doctorat/connectome/wandb/run-20240323_174416-bap0dfep</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/eudald/adult_connectome/runs/bap0dfep' target=\"_blank\">feasible-sound-61</a></strong> to <a href='https://wandb.ai/eudald/adult_connectome' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/eudald/adult_connectome' target=\"_blank\">https://wandb.ai/eudald/adult_connectome</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/eudald/adult_connectome/runs/bap0dfep' target=\"_blank\">https://wandb.ai/eudald/adult_connectome/runs/bap0dfep</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 91/8400 [16:54<36:02:16, 15.61s/it]wandb: ERROR Summary data exceeds maximum size of 10.4MB. Dropping it.\n",
      "  3%|▎         | 237/8400 [1:16:54<44:08:50, 19.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m wandb_:\n\u001B[1;32m     40\u001B[0m     wandb\u001B[38;5;241m.\u001B[39mlog({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m: loss\u001B[38;5;241m.\u001B[39mitem()})\n\u001B[0;32m---> 41\u001B[0m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m scaler\u001B[38;5;241m.\u001B[39mstep(optimizer)\n\u001B[1;32m     43\u001B[0m scaler\u001B[38;5;241m.\u001B[39mupdate()\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if wandb_:\n",
    "    wandb.init(project=\"adult_connectome\", config={\"learning_rate\": lr, \"batch_size\": batch_size})\n",
    "\n",
    "already_selected = []\n",
    "for i in tqdm(range(len(all_videos) // batch_size)):\n",
    "    batch_files, already_selected = select_random_videos(all_videos, batch_size, already_selected)\n",
    "    labels = paths_to_labels(batch_files)  # Assuming this function converts paths to labels\n",
    "    batch_sequences = load_custom_sequences(batch_files)\n",
    "    \n",
    "    # Assuming receptors is a function that processes your sequences\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "    \n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(LayerActivity(simulation, network.connectome, keepref=True))\n",
    "        \n",
    "    del rendered_sequences, simulation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Assuming from_retina_to_model prepares your data for the model, including labels\n",
    "    inputs, labels = from_retina_to_model(\n",
    "        layer_activations, labels, DECODING_CELLS, last_good_frame, classification, root_id_to_index\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    if wandb_:\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    # Update the code to process the entire batch at once\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    labels = labels.to(DEVICE).unsqueeze(-1).float() \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(device_type):\n",
    "        out = model(inputs)\n",
    "        loss = criterion(out, labels)\n",
    "    \n",
    "    if wandb_:\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if debugging and i == 499:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T18:01:15.799214Z",
     "start_time": "2024-03-23T16:44:15.197888Z"
    }
   },
   "id": "f8fa2f7661646a93",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "already_selected_validation = []\n",
    "# Assuming batch_size is defined\n",
    "for _ in tqdm(range(len(all_validation_videos) // batch_size)):\n",
    "    batch_files, already_selected_validation = select_random_videos(all_validation_videos, batch_size, already_selected_validation)\n",
    "\n",
    "    labels = paths_to_labels(batch_files)  # Convert paths to labels\n",
    "    batch_sequences = load_custom_sequences(batch_files)  # Load and preprocess the video sequences\n",
    "    \n",
    "    # Assuming receptors is a function that processes your sequences\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "    \n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(LayerActivity(simulation, network.connectome, keepref=True))\n",
    "        \n",
    "    del rendered_sequences, simulation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Preparing the data for the model, similar to training\n",
    "    inputs, labels = from_retina_to_model(layer_activations, labels, DECODING_CELLS, last_good_frame, classification, root_id_to_index)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).unsqueeze(-1).float()\n",
    "        \n",
    "        with autocast(device_type):\n",
    "            predictions = model(inputs)\n",
    "            # Assuming your criterion and evaluation metrics are defined similarly to training\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            # Calculate other metrics if necessary, e.g., accuracy\n",
    "            \n",
    "            # Log validation metrics to WandB\n",
    "            wandb.log({\"validation_loss\": loss.item()})\n",
    "            # Log other metrics similarly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33a081fe1b9a883e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e93414a83f7bbd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When cuda breaks:\n",
    "sudo rmmod nvidia_uvm\n",
    "sudo modprobe nvidia_uvm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dabc4d2827d8cc4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ba39b3687e323bec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
