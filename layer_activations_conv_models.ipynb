{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:50:38.096103Z",
     "start_time": "2024-04-08T10:50:38.086774Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import device, cuda, autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import flyvision\n",
    "from flyvision.utils.activity_utils import LayerActivity\n",
    "from flyvision_ans import DECODING_CELLS\n",
    "import wandb\n",
    "from conv_models import DecodingImagesCNN\n",
    "from from_retina_to_connectome_utils import layer_activations_to_decoding_images, predictions_and_corrects_from_model_results, update_results_df, update_running_loss, initialize_results_df\n",
    "from from_video_to_training_batched_funcs import get_files_from_directory, select_random_videos, paths_to_labels\n",
    "from from_image_to_video import image_paths_to_sequences\n",
    "from utils import plot_weber_fraction\n",
    "from logs_to_wandb import log_running_stats_to_wandb, log_validation_stats_to_wandb\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='invalid value encountered in cast',\n",
    "    category=RuntimeWarning,\n",
    "    module='wandb.sdk.data_types.image'\n",
    ")\n",
    "\n",
    "device_type = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "DEVICE = device(device_type)\n",
    "torch.manual_seed(42)\n",
    "batch_size = 64\n",
    "last_good_frame = 2\n",
    "\n",
    "TRAINING_DATA_DIR = \"videos/easy_videos\"\n",
    "TESTING_DATA_DIR = \"images/easyval_images\"\n",
    "\n",
    "debugging = False\n",
    "debug_length = 20\n",
    "wandb_ = True\n",
    "wandb_images_every = 10\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "channel_sizes = [16, 32, 64]\n",
    "mult_size = 1\n",
    "channel_sizes = [int(a * mult_size) for a in channel_sizes]\n",
    "dropout = .5\n",
    "lr = 5e-3\n",
    "weight_decay = 5e-3\n",
    "\n",
    "model_config = {\n",
    "    \"debugging\": debugging,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"channel_sizes\": channel_sizes,\n",
    "    \"dropout\": dropout,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "1807fd157955627",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T10:50:50.040082Z",
     "start_time": "2024-04-08T10:50:45.037320Z"
    }
   },
   "source": [
    "network_view = flyvision.NetworkView(flyvision.results_dir / \"opticflow/000/0000\")\n",
    "network = network_view.init_network(chkpt=\"best_chkpt\")\n",
    "dt = 1 / 100\n",
    "extent, kernel_size = 15, 13\n",
    "receptors = flyvision.rendering.BoxEye(extent=extent, kernel_size=kernel_size)\n",
    "\n",
    "all_videos = get_files_from_directory(TRAINING_DATA_DIR)\n",
    "all_validation_videos = get_files_from_directory(TESTING_DATA_DIR)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "4fce80a3502cf3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T10:50:51.826363Z",
     "start_time": "2024-04-08T10:50:51.819659Z"
    }
   },
   "source": [
    "model = DecodingImagesCNN(\n",
    "    out_channels_1 = channel_sizes[0],\n",
    "    out_channels_2 = channel_sizes[1],\n",
    "    out_channels_3 = channel_sizes[2],\n",
    "    dropout = dropout\n",
    ").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = BCEWithLogitsLoss()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "a3b94263294e5262",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T10:55:53.864932Z",
     "start_time": "2024-04-08T10:50:53.604442Z"
    }
   },
   "source": [
    "from from_video_to_training_batched_funcs import load_custom_sequences\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Start wandb run\n",
    "if wandb_:\n",
    "    wandb.init(project='la_classification', config=model_config)\n",
    "\n",
    "results = initialize_results_df()\n",
    "\n",
    "# Training loop\n",
    "iterations = debug_length if debugging else len(all_videos) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    already_selected = []\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for i in tqdm(range(iterations)):\n",
    "        batch_files, already_selected = select_random_videos(\n",
    "            all_videos, batch_size, already_selected\n",
    "        )\n",
    "        labels = paths_to_labels(batch_files)\n",
    "        batch_sequences = load_custom_sequences(batch_files)\n",
    "        # batch_sequences = image_paths_to_sequences(batch_files)\n",
    "        rendered_sequences = receptors(batch_sequences)\n",
    "        \n",
    "        layer_activations = []\n",
    "        for rendered_sequence in rendered_sequences:\n",
    "            # rendered sequences are in RGB; move it to 0-1 for better training\n",
    "            rendered_sequence = torch.div(rendered_sequence, 255)\n",
    "            # TODO: try to run this on cpu to parallelize it\n",
    "            simulation = network.simulate(rendered_sequence[None], dt)\n",
    "            layer_activations.append(\n",
    "                LayerActivity(simulation, network.connectome, keepref=True)\n",
    "            )\n",
    "        \n",
    "        decoding_images = layer_activations_to_decoding_images(layer_activations, last_good_frame, DECODING_CELLS)\n",
    "        \n",
    "        # Ensure the data is in tensor form and on the correct device\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float).to(DEVICE)\n",
    "        batch_sequences_tensor = torch.tensor(decoding_images, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "        # Create the dataset and dataloader\n",
    "        dataset = TensorDataset(batch_sequences_tensor, labels_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for inputs, batch_labels in dataloader:\n",
    "            inputs.to(DEVICE)\n",
    "            batch_labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            with autocast(device_type):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                # Calculate run parameters\n",
    "                predictions, batch_labels_cpu, correct = predictions_and_corrects_from_model_results(outputs, batch_labels)\n",
    "                results = update_results_df(results, batch_files, predictions, batch_labels_cpu, correct)\n",
    "                running_loss += update_running_loss(loss, inputs)\n",
    "                total += batch_labels.shape[0]\n",
    "                total_correct += correct.sum()\n",
    "                \n",
    "        # Log metrics to wandb\n",
    "        if wandb_:\n",
    "            log_running_stats_to_wandb(epoch, i, running_loss, total_correct, total, results)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / total}, Accuracy: {total_correct / total}')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:kj6e0wbh) before initializing another..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.125 MB of 0.125 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9e090201ece484d93a0bc34e77e8a7c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "W&B sync reduced upload amount by 2.1%             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▄▅█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁</td></tr><tr><td>iteration</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▃▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.45937</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>iteration</td><td>4</td></tr><tr><td>loss</td><td>0.75511</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-moon-28</strong> at: <a href='https://wandb.ai/eudald/la_classification/runs/kj6e0wbh' target=\"_blank\">https://wandb.ai/eudald/la_classification/runs/kj6e0wbh</a><br/> View project at: <a href='https://wandb.ai/eudald/la_classification' target=\"_blank\">https://wandb.ai/eudald/la_classification</a><br/>Synced 5 W&B file(s), 5 media file(s), 7 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_124442-kj6e0wbh/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:kj6e0wbh). Initializing new run:<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112188844445982, max=1.0…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b9f89eff9814ad18e286a18a4424e3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/eudald/Desktop/doctorat/connectome/wandb/run-20240408_125053-xxe5wdrq</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eudald/la_classification/runs/xxe5wdrq' target=\"_blank\">faithful-silence-29</a></strong> to <a href='https://wandb.ai/eudald/la_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/eudald/la_classification' target=\"_blank\">https://wandb.ai/eudald/la_classification</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/eudald/la_classification/runs/xxe5wdrq' target=\"_blank\">https://wandb.ai/eudald/la_classification/runs/xxe5wdrq</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/262 [04:49<4:07:45, 57.84s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     30\u001B[0m     rendered_sequence \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdiv(rendered_sequence, \u001B[38;5;241m255\u001B[39m)\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;66;03m# TODO: try to run this on cpu to parallelize it\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m     simulation \u001B[38;5;241m=\u001B[39m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimulate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrendered_sequence\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     layer_activations\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m     34\u001B[0m         LayerActivity(simulation, network\u001B[38;5;241m.\u001B[39mconnectome, keepref\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     35\u001B[0m     )\n\u001B[1;32m     37\u001B[0m decoding_images \u001B[38;5;241m=\u001B[39m layer_activations_to_decoding_images(layer_activations, last_good_frame, DECODING_CELLS)\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:505\u001B[0m, in \u001B[0;36mNetwork.simulate\u001B[0;34m(self, movie_input, dt, initial_state, as_states)\u001B[0m\n\u001B[1;32m    503\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstimulus\u001B[38;5;241m.\u001B[39mzero(batch_size, n_frames)\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstimulus\u001B[38;5;241m.\u001B[39madd_input(movie_input)\n\u001B[0;32m--> 505\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstimulus\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m movie_input\n\u001B[1;32m    507\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:547\u001B[0m, in \u001B[0;36mNetwork.forward\u001B[0;34m(self, x, dt, state, as_states)\u001B[0m\n\u001B[1;32m    544\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m as_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    545\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(handle(state))\n\u001B[0;32m--> 547\u001B[0m state_stack \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    548\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m x, state\n\u001B[1;32m    549\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:538\u001B[0m, in \u001B[0;36mNetwork.forward.<locals>.handle\u001B[0;34m(state)\u001B[0m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandle\u001B[39m(state):\n\u001B[1;32m    536\u001B[0m     \u001B[38;5;66;03m# loop over the temporal dimension for integration of dynamics\u001B[39;00m\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]):\n\u001B[0;32m--> 538\u001B[0m         state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    539\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m as_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    540\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m state\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mactivity\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:383\u001B[0m, in \u001B[0;36mNetwork._next_state\u001B[0;34m(self, params, state, x_t, dt)\u001B[0m\n\u001B[1;32m    368\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute the next state, given the current `state` and stimulus `x_t`.\u001B[39;00m\n\u001B[1;32m    369\u001B[0m \n\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;124;03mNote: simple, elementwise Euler integration.\u001B[39;00m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    381\u001B[0m vel \u001B[38;5;241m=\u001B[39m AutoDeref(nodes\u001B[38;5;241m=\u001B[39mAutoDeref(), edges\u001B[38;5;241m=\u001B[39mAutoDeref())\n\u001B[0;32m--> 383\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdynamics\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_state_velocity\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_sum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdt\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    387\u001B[0m next_state \u001B[38;5;241m=\u001B[39m AutoDeref(\n\u001B[1;32m    388\u001B[0m     nodes\u001B[38;5;241m=\u001B[39mAutoDeref(\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{k: state\u001B[38;5;241m.\u001B[39mnodes[k] \u001B[38;5;241m+\u001B[39m vel\u001B[38;5;241m.\u001B[39mnodes[k] \u001B[38;5;241m*\u001B[39m dt \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m state\u001B[38;5;241m.\u001B[39mnodes}\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    393\u001B[0m     ),\n\u001B[1;32m    394\u001B[0m )\n\u001B[1;32m    396\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_api(next_state)\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/dynamics.py:172\u001B[0m, in \u001B[0;36mPPNeuronIGRSynapses.write_state_velocity\u001B[0;34m(self, vel, state, params, target_sum, x_t, dt, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrite_state_velocity\u001B[39m(\u001B[38;5;28mself\u001B[39m, vel, state, params, target_sum, x_t, dt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    169\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Velocity is the bias plus the sum of the weighted rectified inputs.\"\"\"\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     vel\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mactivity \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    171\u001B[0m         \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 172\u001B[0m         \u001B[38;5;241m/\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(params\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mtime_const, \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[1;32m    173\u001B[0m         \u001B[38;5;241m*\u001B[39m (\n\u001B[1;32m    174\u001B[0m             \u001B[38;5;241m-\u001B[39mstate\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mactivity\n\u001B[1;32m    175\u001B[0m             \u001B[38;5;241m+\u001B[39m params\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mbias\n\u001B[1;32m    176\u001B[0m             \u001B[38;5;241m+\u001B[39m target_sum(\n\u001B[1;32m    177\u001B[0m                 params\u001B[38;5;241m.\u001B[39medges\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(state\u001B[38;5;241m.\u001B[39msources\u001B[38;5;241m.\u001B[39mactivity)\n\u001B[1;32m    178\u001B[0m             )  \u001B[38;5;66;03m# internal chemical current\u001B[39;00m\n\u001B[1;32m    179\u001B[0m             \u001B[38;5;241m+\u001B[39m x_t\n\u001B[1;32m    180\u001B[0m         )\n\u001B[1;32m    181\u001B[0m     )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "15804021f0e43b93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T06:55:32.996401Z",
     "start_time": "2024-04-08T06:39:46.261719Z"
    }
   },
   "source": [
    "model.eval() \n",
    "\n",
    "total_correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "validation_results = initialize_results_df()\n",
    "\n",
    "with (torch.no_grad()):\n",
    "    for i in tqdm(range(len(all_validation_videos) // batch_size)):\n",
    "        batch_files, _ = select_random_videos(\n",
    "            all_validation_videos, batch_size, already_selected=[]\n",
    "        )\n",
    "        labels = paths_to_labels(batch_files)\n",
    "        batch_sequences = image_paths_to_sequences(batch_files)\n",
    "        rendered_sequences = receptors(batch_sequences)\n",
    "        \n",
    "        layer_activations = []\n",
    "        for rendered_sequence in rendered_sequences:\n",
    "            # rendered sequences are in RGB; move it to 0-1 for better training\n",
    "            rendered_sequence = torch.div(rendered_sequence, 255)\n",
    "            simulation = network.simulate(rendered_sequence[None], dt)\n",
    "            layer_activations.append(\n",
    "                LayerActivity(simulation, network.connectome, keepref=True)\n",
    "            )\n",
    "        \n",
    "        decoding_images = layer_activations_to_decoding_images(layer_activations, last_good_frame, DECODING_CELLS)\n",
    "        \n",
    "        # Ensure the data is in tensor form and on the correct device\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float).to(DEVICE)\n",
    "        batch_sequences_tensor = torch.tensor(decoding_images, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "        dataset = TensorDataset(batch_sequences_tensor, labels_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for inputs, batch_labels in dataloader:\n",
    "            inputs, batch_labels = inputs.to(DEVICE), batch_labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), batch_labels)\n",
    "\n",
    "            predictions, batch_labels_cpu, correct = predictions_and_corrects_from_model_results(outputs, batch_labels)\n",
    "            validation_results = update_results_df(validation_results, batch_files, predictions, batch_labels_cpu, correct)\n",
    "            running_loss += update_running_loss(loss, inputs)\n",
    "            total += batch_labels_cpu.shape[0]\n",
    "            total_correct += correct.sum().item()\n",
    "            \n",
    "            \n",
    "print(f'Validation Loss: {running_loss / total}, '\n",
    "      f'Validation Accuracy: {total_correct / total}')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [15:46<00:00, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.200814972158338, Validation Accuracy: 0.49001024590163933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "6a25342c29d5d247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T06:56:57.252140Z",
     "start_time": "2024-04-08T06:56:57.231623Z"
    }
   },
   "source": [
    "weber_plot = plot_weber_fraction(validation_results)\n",
    "\n",
    "# Log validation metrics to wandb\n",
    "if wandb_:\n",
    "    log_validation_stats_to_wandb(running_loss, total_correct, total, validation_results, weber_plot)\n",
    "    wandb.finish()"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m weber_plot \u001B[38;5;241m=\u001B[39m \u001B[43mplot_weber_fraction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_results\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Log validation metrics to wandb\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m wandb_:\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/utils.py:41\u001B[0m, in \u001B[0;36mplot_weber_fraction\u001B[0;34m(results_df)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_weber_fraction\u001B[39m(results_df: pd\u001B[38;5;241m.\u001B[39mDataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m plt\u001B[38;5;241m.\u001B[39mFigure:\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;66;03m# Calculate the percentage of correct answers for each Weber ratio\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m     results_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myellow\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mresults_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mImage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m     42\u001B[0m     results_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblue\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m results_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m3\u001B[39m])\n\u001B[1;32m     43\u001B[0m     results_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweber_ratio\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m results_df\u001B[38;5;241m.\u001B[39mapply(\n\u001B[1;32m     44\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m row: \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myellow\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblue\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n\u001B[1;32m     45\u001B[0m         \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myellow\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblue\u001B[39m\u001B[38;5;124m\"\u001B[39m])),\n\u001B[1;32m     46\u001B[0m         axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     47\u001B[0m     )\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T06:57:46.565470Z",
     "start_time": "2024-04-08T06:57:46.561960Z"
    }
   },
   "cell_type": "code",
   "source": "validation_results",
   "id": "7fa3503bd9d894eb",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd691e219cd4632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:25:28.370784Z",
     "start_time": "2024-04-03T13:25:28.343060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "          Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "              Conv2d-1     [1, 34, 31, 31]           4,912           4,912\n",
      "         BatchNorm2d-2     [1, 16, 31, 31]              32              32\n",
      "             Dropout-3     [1, 16, 15, 15]               0               0\n",
      "              Conv2d-4     [1, 16, 15, 15]           4,640           4,640\n",
      "         BatchNorm2d-5     [1, 32, 15, 15]              64              64\n",
      "              Conv2d-6       [1, 32, 7, 7]          18,496          18,496\n",
      "         BatchNorm2d-7       [1, 64, 7, 7]             128             128\n",
      "   AdaptiveAvgPool2d-8       [1, 64, 3, 3]               0               0\n",
      "              Linear-9             [1, 64]              65              65\n",
      "===========================================================================\n",
      "Total params: 28,337\n",
      "Trainable params: 28,337\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pytorch_model_summary import summary\n",
    "\n",
    "print(summary(DecodingImagesCNN(), torch.zeros((1, 34, 31, 31)), show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "id": "37944a092e7d3e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T10:56:11.095960Z",
     "start_time": "2024-04-08T10:56:11.091774Z"
    }
   },
   "source": "torch.cuda.is_available()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a01a5867d81a9cc0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
