{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:10.443650381Z",
     "start_time": "2024-02-02T16:08:06.722907340Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from retina_to_connectome import get_activation_tensor, get_batch_voronoi_averages\n",
    "\n",
    "\n",
    "from flyvis.examples.flyvision_ans import DECODING_CELLS\n",
    "\n",
    "last_good_frame = 8"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get data\n",
    "activations_dir = \"flyvis/parsed_objects\"\n",
    "activations = np.load(os.path.join(activations_dir, \"decoding_activations.npy\"), allow_pickle=True)\n",
    "classification = pd.read_csv(\"adult_data/classification.csv\")\n",
    "\n",
    "# remove duplicated root_ids\n",
    "classification = classification.drop_duplicates(subset='root_id')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:12.078660353Z",
     "start_time": "2024-02-02T16:08:10.443645880Z"
    }
   },
   "id": "a1466ccb6d444b07",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:10<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "avgs_dict = {}\n",
    "for cell_type in tqdm(DECODING_CELLS):\n",
    "    number_of_cells = len(classification[classification[\"cell_type\"] == cell_type])\n",
    "    if number_of_cells > 0:\n",
    "        activation_tensor = get_activation_tensor(activations, cell_type, last_frame=last_good_frame) / 255\n",
    "        avgs_dict[cell_type] = get_batch_voronoi_averages(activation_tensor, n_centers=number_of_cells)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:22.994877768Z",
     "start_time": "2024-02-02T16:08:12.082151241Z"
    }
   },
   "id": "8a91bc55a5c9604c",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def voronoi_averages_to_df(dict_with_voronoi_averages):\n",
    "    dfs = []\n",
    "    for key, matrix in dict_with_voronoi_averages.items():\n",
    "        df = pd.DataFrame(matrix.transpose())\n",
    "        df['index_name'] = key\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all the DataFrames into one\n",
    "    return pd.concat(dfs, axis=0, ignore_index=True)\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:28.269595524Z",
     "start_time": "2024-02-02T16:08:28.223839770Z"
    }
   },
   "id": "16ba36ae0792bc06",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "result_df = voronoi_averages_to_df(avgs_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:28.327991824Z",
     "start_time": "2024-02-02T16:08:28.249391908Z"
    }
   },
   "id": "66d6df94b20c7524",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extract cell types and activations\n",
    "cell_types = result_df.iloc[:, -1]  # Last column for cell type\n",
    "activations = result_df.iloc[:, :-1]  # Exclude the last column\n",
    "\n",
    "# Create a dictionary to hold shuffled root_ids for each cell type\n",
    "root_id_mapping = {}\n",
    "\n",
    "# Populate the dictionary with shuffled root_ids for each cell type\n",
    "for cell_type, group in classification.groupby(\"cell_type\"):\n",
    "    # Shuffle the root_ids within each group\n",
    "    shuffled_root_ids = group['root_id'].sample(frac=1).values\n",
    "    root_id_mapping[cell_type] = shuffled_root_ids\n",
    "\n",
    "# Function to assign root_ids to each row in result_df based on cell type and available root_ids\n",
    "def assign_root_ids(row):\n",
    "    cell_type = row.iloc[-1]  # Get cell type from the last column\n",
    "    # Get the list of shuffled root_ids for this cell type\n",
    "    root_ids = root_id_mapping[cell_type]\n",
    "    # Assign a root_id from the list, ensuring we don't exceed the list's length\n",
    "    # The index in the list is the count of occurrences of this cell type so far, modulo the number of available root_ids\n",
    "    root_id_index = row.name % len(root_ids)  # row.name is the index of the row in the dataframe\n",
    "    return root_ids[root_id_index]\n",
    "\n",
    "# Apply the function to result_df, creating a new 'root_id' column\n",
    "result_df['root_id'] = result_df.apply(assign_root_ids, axis=1)\n",
    "\n",
    "# Remove duplicated root_ids\n",
    "result_df = result_df.drop_duplicates(subset='root_id')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:35.023866028Z",
     "start_time": "2024-02-02T16:08:34.107380352Z"
    }
   },
   "id": "aa33a3309a235441",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "activation_df = pd.merge(\n",
    "    classification.drop(\n",
    "        columns=[\"flow\", \"super_class\", \"class\", \"sub_class\", \n",
    "                 \"hemibrain_type\", \"hemilineage\", \"side\", \"nerve\"]), \n",
    "    result_df.drop(columns=[result_df.columns[-2]]), on='root_id', how='left').fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:38.422438520Z",
     "start_time": "2024-02-02T16:08:38.232840838Z"
    }
   },
   "id": "7b9d37652bf3d75a",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from adult_models_helpers import get_synapse_df\n",
    "synapse_df = get_synapse_df()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:41.190286576Z",
     "start_time": "2024-02-02T16:08:38.572771830Z"
    }
   },
   "id": "ea5873de3ea67ca8",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1: Identify Common Neurons\n",
    "# Unique root_ids in merged_df\n",
    "neurons_merged = pd.unique(activation_df['root_id'])\n",
    "\n",
    "# Unique root_ids in synapse_df (both pre and post)\n",
    "neurons_synapse_pre = pd.unique(synapse_df['pre_root_id'])\n",
    "neurons_synapse_post = pd.unique(synapse_df['post_root_id'])\n",
    "neurons_synapse = np.unique(np.concatenate([neurons_synapse_pre, neurons_synapse_post]))\n",
    "\n",
    "# Common neurons\n",
    "common_neurons = np.intersect1d(neurons_merged, neurons_synapse)\n",
    "\n",
    "# Step 2: Filter synapse_df\n",
    "# Keep only rows with both pre and post root_ids in common_neurons\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Filter synapse_df to include only rows with both pre and post root_ids in common_neurons\n",
    "filtered_synapse_df = synapse_df[\n",
    "    synapse_df['pre_root_id'].isin(common_neurons) & synapse_df['post_root_id'].isin(common_neurons)\n",
    "]\n",
    "\n",
    "# Map neuron root_ids to matrix indices\n",
    "root_id_to_index = {root_id: index for index, root_id in enumerate(common_neurons)}\n",
    "\n",
    "# Convert root_ids in filtered_synapse_df to matrix indices\n",
    "pre_indices = filtered_synapse_df['pre_root_id'].map(root_id_to_index).values\n",
    "post_indices = filtered_synapse_df['post_root_id'].map(root_id_to_index).values\n",
    "\n",
    "# Use syn_count as the data for the non-zero elements of the matrix\n",
    "data = filtered_synapse_df['syn_count'].values\n",
    "\n",
    "# Create a sparse matrix in COO format\n",
    "synaptic_matrix_sparse = coo_matrix(\n",
    "    (data, (pre_indices, post_indices)),\n",
    "    shape=(len(common_neurons), len(common_neurons)),\n",
    "    dtype=np.int64  # or np.float32/np.float64 if memory issue persists\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:43.462857632Z",
     "start_time": "2024-02-02T16:08:42.042187252Z"
    }
   },
   "id": "4b9b0fc3eb3b1c75",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Neurons in merged_df\n",
    "neurons_merged = set(activation_df['root_id'])\n",
    "\n",
    "# Neurons in synaptic_matrix_sparse\n",
    "neurons_synaptic = set(common_neurons)  # common_neurons was used to build the synaptic matrix\n",
    "\n",
    "# Neurons in merged_df not in synaptic_matrix\n",
    "missing_in_synaptic = neurons_merged - neurons_synaptic\n",
    "\n",
    "# Neurons in synaptic_matrix not in merged_df\n",
    "missing_in_merged = neurons_synaptic - neurons_merged"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:46.768942813Z",
     "start_time": "2024-02-02T16:08:46.720924521Z"
    }
   },
   "id": "ac4670dc90dd0a2c",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Dict, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from adult_models_helpers import get_synapse_df\n",
    "\n",
    "\n",
    "class AdultConnectomeNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        adjacency_matrix,\n",
    "        neuron_count: int,\n",
    "        general_config: Dict[str, Union[int, float, str, bool]],\n",
    "    ):\n",
    "        super(AdultConnectomeNetwork, self).__init__()\n",
    "        \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Convert the adjacency matrix to a PyTorch sparse tensor once in the initialization\n",
    "        self.adjacency_matrix_coo = adjacency_matrix.tocoo()\n",
    "        self.adj_matrix_sparse = torch.sparse_coo_tensor(\n",
    "            torch.tensor(\n",
    "                [self.adjacency_matrix_coo.row, self.adjacency_matrix_coo.col]\n",
    "            ),\n",
    "            torch.FloatTensor(self.adjacency_matrix_coo.data),\n",
    "            torch.Size(self.adjacency_matrix_coo.shape),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.connectome_layer_number = general_config[\"CONNECTOME_LAYER_NUMBER\"]\n",
    "\n",
    "        # Initialize the shared weights for the connectome layers\n",
    "        self.shared_weights = self.initialize_sparse_weights(\n",
    "            adjacency_matrix, neuron_count\n",
    "        )\n",
    "        self.shared_bias = nn.Parameter(torch.ones(neuron_count))\n",
    "\n",
    "    def initialize_sparse_weights(self, adjacency_matrix, neuron_count):\n",
    "        # Generate random weights for existing connections, ensuring the tensor is on the same device\n",
    "        weights = torch.rand(\n",
    "            len(adjacency_matrix.data), device=self.device\n",
    "        )  # Specify device here\n",
    "\n",
    "        # Create sparse weights tensor, ensuring indices are on the same device\n",
    "        indices = torch.tensor(\n",
    "            [adjacency_matrix.row, adjacency_matrix.col], device=self.device\n",
    "        )  # Specify device here\n",
    "        sparse_weights = torch.sparse_coo_tensor(\n",
    "            indices, weights, (neuron_count, neuron_count), device=self.device\n",
    "        )\n",
    "\n",
    "        return nn.Parameter(sparse_weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Use the pre-converted adjacency matrix in sparse format\n",
    "        adj_matrix = self.adj_matrix_sparse.to(x.device)\n",
    "\n",
    "        # Pass the input through the layer with shared weights\n",
    "        for _ in range(self.connectome_layer_number):\n",
    "            # Apply the mask from the adjacency matrix to the shared weights\n",
    "            masked_weights = torch.sparse.mm(adj_matrix, self.shared_weights).to(x.device)\n",
    "\n",
    "            # Do the forward pass using sparse matrix multiplication\n",
    "            x = torch.sparse.mm(masked_weights, x) + self.shared_bias.unsqueeze(0)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:47.815228251Z",
     "start_time": "2024-02-02T16:08:47.811795705Z"
    }
   },
   "id": "3727145dbfc1970b",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "activation_df = activation_df[activation_df['root_id'].isin(list(root_id_to_index.keys()))]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:50.906254396Z",
     "start_time": "2024-02-02T16:08:50.880761592Z"
    }
   },
   "id": "ee8423219793cadf",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45370/925662702.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  torch.tensor(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 808.00 MiB. GPU 0 has a total capacty of 7.58 GiB of which 476.00 MiB is free. Including non-PyTorch memory, this process has 6.95 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 53.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 21>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     18\u001B[0m adult_connectome_network\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Now, you can feed the entire batch of samples into the network\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m output_activations \u001B[38;5;241m=\u001B[39m \u001B[43madult_connectome_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactivation_tensor\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36mAdultConnectomeNetwork.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# Pass the input through the layer with shared weights\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnectome_layer_number):\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;66;03m# Apply the mask from the adjacency matrix to the shared weights\u001B[39;00m\n\u001B[0;32m---> 61\u001B[0m     masked_weights \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43madj_matrix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshared_weights\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;66;03m# Do the forward pass using sparse matrix multiplication\u001B[39;00m\n\u001B[1;32m     64\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msparse\u001B[38;5;241m.\u001B[39mmm(masked_weights, x) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshared_bias\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 808.00 MiB. GPU 0 has a total capacty of 7.58 GiB of which 476.00 MiB is free. Including non-PyTorch memory, this process has 6.95 GiB memory in use. Of the allocated memory 6.74 GiB is allocated by PyTorch, and 53.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "activation_data = activation_df.drop(columns=[\"root_id\", \"cell_type\"])\n",
    "\n",
    "# Convert the activation data DataFrame to a numpy array, then to a PyTorch tensor\n",
    "activation_tensor = torch.tensor(activation_data.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Transpose the tensor to have the sample dimension first (sample_size x neuron_count)\n",
    "activation_tensor = activation_tensor.t()\n",
    "\n",
    "sample_count, neuron_count = activation_tensor.shape\n",
    "general_config = {\"CONNECTOME_LAYER_NUMBER\": 2}\n",
    "\n",
    "adult_connectome_network = AdultConnectomeNetwork(synaptic_matrix_sparse, neuron_count, general_config)\n",
    "adult_connectome_network.to(device)\n",
    "\n",
    "# Now, you can feed the entire batch of samples into the network\n",
    "output_activations = adult_connectome_network(activation_tensor)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T16:08:58.084829716Z",
     "start_time": "2024-02-02T16:08:55.433743292Z"
    }
   },
   "id": "9502729772032235",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from utils import flush_cuda_memory\n",
    "\n",
    "flush_cuda_memory()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T14:21:49.052533048Z",
     "start_time": "2024-02-02T14:21:48.973473992Z"
    }
   },
   "id": "3db8404555c0d8bd",
   "execution_count": 145
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# find duplicated root_ids in the activation_df\n",
    "duplicated_root_ids = activation_df[activation_df.duplicated(subset='root_id', keep=False)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T11:14:50.879821952Z",
     "start_time": "2024-02-02T11:14:50.619012615Z"
    }
   },
   "id": "2ceaab1d8154f7a4",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                   root_id cell_type         0         1         2         3  \\\n294     720575940626442596       Tm2 -0.274404 -0.269373 -0.277603 -0.268852   \n295     720575940626442596       Tm2       Tm2       Tm2       Tm2       Tm2   \n16391   720575940631359412     TmY15  0.290887   0.30118  0.309714  0.293187   \n16392   720575940631359412     TmY15     TmY15     TmY15     TmY15     TmY15   \n35210   720575940622440572      TmY3 -0.536687 -0.540938 -0.505989 -0.538004   \n35211   720575940622440572      TmY3      TmY3      TmY3      TmY3      TmY3   \n36116   720575940627533992     TmY18  0.455759  0.444986  0.456813  0.422267   \n36117   720575940627533992     TmY18     TmY18     TmY18     TmY18     TmY18   \n41865   720575940627442831       T4c -0.027249 -0.027896 -0.025447 -0.036456   \n41866   720575940627442831       T4c       T4c       T4c       T4c       T4c   \n42974   720575940613432547        T2  1.643621  1.634292  1.664171  1.644273   \n42975   720575940613432547        T2        T2        T2        T2        T2   \n43964   720575940612429938       Tm1  0.006562   0.00664  0.006699    0.0066   \n43965   720575940612429938       Tm1       Tm1       Tm1       Tm1       Tm1   \n52870   720575940615538847     TmY14 -0.470678 -0.485449 -0.486087 -0.487877   \n52871   720575940615538847     TmY14     TmY14     TmY14     TmY14     TmY14   \n53868   720575940631272908       Tm9 -0.230167 -0.236416 -0.236989 -0.228967   \n53869   720575940631272908       Tm9       Tm9       Tm9       Tm9       Tm9   \n54704   720575940623153961       T4b  0.027528   0.02611    0.0311  0.027853   \n54705   720575940623153961       T4b       T4b       T4b       T4b       T4b   \n54826   720575940629147445       Tm4 -0.167922 -0.163578 -0.164672 -0.165606   \n54827   720575940629147445       Tm4       Tm4       Tm4       Tm4       Tm4   \n59553   720575940615205031      TmY9  0.270779  0.270226  0.267313  0.267039   \n59554   720575940615205031      TmY9      TmY9      TmY9      TmY9      TmY9   \n62921   720575940644202696     TmY5a  0.024819  0.019574  0.021826   0.02261   \n62922   720575940644202696     TmY5a     TmY5a     TmY5a     TmY5a     TmY5a   \n65086   720575940633128751      Tm20 -0.012243 -0.011983 -0.011619 -0.011609   \n65087   720575940633128751      Tm20      Tm20      Tm20      Tm20      Tm20   \n70150   720575940639020351       T4d -0.228009 -0.212095 -0.246063 -0.228065   \n70151   720575940639020351       T4d       T4d       T4d       T4d       T4d   \n72259   720575940628618106       T5b -0.113973 -0.114224 -0.114975 -0.116502   \n72260   720575940628618106       T5b       T5b       T5b       T5b       T5b   \n79206   720575940629907906      Tm5Y  -0.08547 -0.086101 -0.087118 -0.084572   \n79207   720575940629907906      Tm5Y      Tm5Y      Tm5Y      Tm5Y      Tm5Y   \n80237   720575940614015981        T1 -0.213574 -0.213596 -0.208285 -0.209546   \n80238   720575940614015981        T1        T1        T1        T1        T1   \n103550  720575940614218407        T3  0.309847  0.302557  0.300639  0.306294   \n103551  720575940614218407        T3        T3        T3        T3        T3   \n114734  720575940625277710       T2a  0.239278  0.224761  0.224668  0.218674   \n114735  720575940625277710       T2a       T2a       T2a       T2a       T2a   \n115847  720575940612468965       Tm3  0.440971  0.442472  0.426341  0.441889   \n115848  720575940612468965       Tm3       Tm3       Tm3       Tm3       Tm3   \n116443  720575940615435926      Tm16 -0.250778 -0.250813 -0.250809 -0.250606   \n116444  720575940615435926      Tm16      Tm16      Tm16      Tm16      Tm16   \n117508  720575940618950401      TmY4 -0.165038 -0.164958 -0.172305 -0.164963   \n117509  720575940618950401      TmY4      TmY4      TmY4      TmY4      TmY4   \n118855  720575940630995983       T5c -0.191226 -0.186858  -0.19287 -0.189666   \n118856  720575940630995983       T5c       T5c       T5c       T5c       T5c   \n126700  720575940620108411       T5d -0.168876 -0.167089 -0.168554 -0.169222   \n126701  720575940620108411       T5d       T5d       T5d       T5d       T5d   \n130001  720575940636345967      Tm5a -0.248215 -0.217244 -0.217475 -0.217276   \n130002  720575940636345967      Tm5a      Tm5a      Tm5a      Tm5a      Tm5a   \n134939  720575940618936733       T4a  0.041009  0.043589  0.046163  0.048303   \n134940  720575940618936733       T4a       T4a       T4a       T4a       T4a   \n138452  720575940610765124      Tm5c  0.361797  0.360946  0.358235  0.373954   \n138453  720575940610765124      Tm5c      Tm5c      Tm5c      Tm5c      Tm5c   \n138640  720575940621298949       T5a -0.116278 -0.116406 -0.120942 -0.110424   \n138641  720575940621298949       T5a       T5a       T5a       T5a       T5a   \n\n               4         5         6         7  ...        90        91  \\\n294    -0.275545 -0.273812 -0.276923 -0.274063  ... -0.281386 -0.273345   \n295          Tm2       Tm2       Tm2       Tm2  ...       Tm2       Tm2   \n16391   0.284795  0.290941  0.275176  0.291664  ...  0.292876  0.296338   \n16392      TmY15     TmY15     TmY15     TmY15  ...     TmY15     TmY15   \n35210   -0.53686 -0.544931 -0.565986 -0.537135  ... -0.543615 -0.531848   \n35211       TmY3      TmY3      TmY3      TmY3  ...      TmY3      TmY3   \n36116    0.45365  0.456177  0.452744  0.453379  ...  0.435793  0.454588   \n36117      TmY18     TmY18     TmY18     TmY18  ...     TmY18     TmY18   \n41865  -0.022821 -0.021151 -0.033814 -0.027082  ... -0.033502 -0.015781   \n41866        T4c       T4c       T4c       T4c  ...       T4c       T4c   \n42974   1.632857  1.648688   1.63838  1.645779  ...  1.617326  1.632607   \n42975         T2        T2        T2        T2  ...        T2        T2   \n43964   0.007688   0.00672  0.005995  0.006526  ...  0.005806  0.007109   \n43965        Tm1       Tm1       Tm1       Tm1  ...       Tm1       Tm1   \n52870  -0.463684 -0.487647 -0.491936 -0.486837  ... -0.488204 -0.480616   \n52871      TmY14     TmY14     TmY14     TmY14  ...     TmY14     TmY14   \n53868  -0.230719 -0.245778 -0.235024  -0.25435  ... -0.219087 -0.236078   \n53869        Tm9       Tm9       Tm9       Tm9  ...       Tm9       Tm9   \n54704   0.021825  0.017837  0.030688   0.02708  ...  0.021439  0.024752   \n54705        T4b       T4b       T4b       T4b  ...       T4b       T4b   \n54826  -0.161746 -0.169276 -0.164354 -0.163264  ... -0.163247 -0.164718   \n54827        Tm4       Tm4       Tm4       Tm4  ...       Tm4       Tm4   \n59553   0.268321  0.268815  0.267335   0.27023  ...  0.265036  0.269848   \n59554       TmY9      TmY9      TmY9      TmY9  ...      TmY9      TmY9   \n62921   0.022273  0.022178  0.019672  0.022927  ...  0.025591  0.013931   \n62922      TmY5a     TmY5a     TmY5a     TmY5a  ...     TmY5a     TmY5a   \n65086  -0.010821 -0.011556 -0.012491 -0.011911  ... -0.011725 -0.011131   \n65087       Tm20      Tm20      Tm20      Tm20  ...      Tm20      Tm20   \n70150  -0.233154 -0.233806 -0.224727 -0.215198  ... -0.244032 -0.228817   \n70151        T4d       T4d       T4d       T4d  ...       T4d       T4d   \n72259  -0.114336 -0.112988 -0.113027 -0.116325  ... -0.113512 -0.114633   \n72260        T5b       T5b       T5b       T5b  ...       T5b       T5b   \n79206  -0.086759 -0.084696 -0.086643 -0.085479  ... -0.084885 -0.084173   \n79207       Tm5Y      Tm5Y      Tm5Y      Tm5Y  ...      Tm5Y      Tm5Y   \n80237  -0.212187 -0.206088 -0.221792 -0.213497  ... -0.209885 -0.209927   \n80238         T1        T1        T1        T1  ...        T1        T1   \n103550  0.301769  0.307941  0.315403  0.311763  ...  0.311101  0.294312   \n103551        T3        T3        T3        T3  ...        T3        T3   \n114734  0.216012  0.225532  0.218613  0.222221  ...  0.230426  0.223746   \n114735       T2a       T2a       T2a       T2a  ...       T2a       T2a   \n115847  0.439991  0.436614  0.438033  0.443253  ...  0.435641  0.425332   \n115848       Tm3       Tm3       Tm3       Tm3  ...       Tm3       Tm3   \n116443 -0.273291 -0.251688 -0.250911 -0.250795  ... -0.229925 -0.202998   \n116444      Tm16      Tm16      Tm16      Tm16  ...      Tm16      Tm16   \n117508 -0.164548 -0.165027 -0.166158 -0.165094  ... -0.164789 -0.165115   \n117509      TmY4      TmY4      TmY4      TmY4  ...      TmY4      TmY4   \n118855 -0.197243 -0.190713  -0.19199 -0.189586  ... -0.191443 -0.187489   \n118856       T5c       T5c       T5c       T5c  ...       T5c       T5c   \n126700 -0.171482 -0.166379 -0.168502 -0.170454  ...  -0.16047 -0.165863   \n126701       T5d       T5d       T5d       T5d  ...       T5d       T5d   \n130001 -0.216703 -0.217197 -0.217233 -0.217113  ... -0.217554 -0.217277   \n130002      Tm5a      Tm5a      Tm5a      Tm5a  ...      Tm5a      Tm5a   \n134939  0.046084  0.041624  0.047951  0.038744  ...  0.048636   0.03479   \n134940       T4a       T4a       T4a       T4a  ...       T4a       T4a   \n138452  0.368736  0.353623  0.329776  0.364594  ...  0.350202  0.345176   \n138453      Tm5c      Tm5c      Tm5c      Tm5c  ...      Tm5c      Tm5c   \n138640  -0.11729 -0.118717   -0.1174 -0.119601  ...  -0.11125 -0.121155   \n138641       T5a       T5a       T5a       T5a  ...       T5a       T5a   \n\n              92        93        94        95        96        97        98  \\\n294    -0.274692 -0.284599 -0.275491 -0.276765 -0.283328 -0.289558 -0.274623   \n295          Tm2       Tm2       Tm2       Tm2       Tm2       Tm2       Tm2   \n16391    0.29092  0.295088  0.286905  0.290851  0.295875  0.296853  0.303404   \n16392      TmY15     TmY15     TmY15     TmY15     TmY15     TmY15     TmY15   \n35210  -0.536896 -0.528557 -0.541048 -0.538066 -0.536549 -0.533267 -0.535469   \n35211       TmY3      TmY3      TmY3      TmY3      TmY3      TmY3      TmY3   \n36116   0.452333  0.452292  0.447814  0.426964  0.448956   0.46067   0.45729   \n36117      TmY18     TmY18     TmY18     TmY18     TmY18     TmY18     TmY18   \n41865  -0.027234 -0.009681 -0.017577  -0.02853 -0.022715 -0.032866 -0.021058   \n41866        T4c       T4c       T4c       T4c       T4c       T4c       T4c   \n42974   1.647115  1.637437  1.643038  1.637348  1.652678  1.624251   1.64793   \n42975         T2        T2        T2        T2        T2        T2        T2   \n43964    0.00668  0.006631  0.006792  0.006399   0.00666  0.006982  0.006732   \n43965        Tm1       Tm1       Tm1       Tm1       Tm1       Tm1       Tm1   \n52870  -0.486542 -0.483265 -0.485588 -0.486988 -0.478718 -0.486942 -0.479125   \n52871      TmY14     TmY14     TmY14     TmY14     TmY14     TmY14     TmY14   \n53868  -0.236573 -0.243764 -0.233919  -0.23549 -0.225156 -0.235632 -0.242537   \n53869        Tm9       Tm9       Tm9       Tm9       Tm9       Tm9       Tm9   \n54704   0.025348  0.025486  0.048752  0.047477  0.031102  0.035537  0.033298   \n54705        T4b       T4b       T4b       T4b       T4b       T4b       T4b   \n54826  -0.164774 -0.164814 -0.167767 -0.165545 -0.165277 -0.166722 -0.165834   \n54827        Tm4       Tm4       Tm4       Tm4       Tm4       Tm4       Tm4   \n59553     0.2737  0.269777  0.276135  0.267619  0.271146  0.270308  0.273503   \n59554       TmY9      TmY9      TmY9      TmY9      TmY9      TmY9      TmY9   \n62921   0.021971  0.020398  0.018514  0.021708  0.024555  0.015851  0.023967   \n62922      TmY5a     TmY5a     TmY5a     TmY5a     TmY5a     TmY5a     TmY5a   \n65086  -0.011791 -0.012418 -0.011989 -0.012521 -0.012037 -0.012996 -0.012268   \n65087       Tm20      Tm20      Tm20      Tm20      Tm20      Tm20      Tm20   \n70150  -0.228752 -0.228597 -0.230503 -0.210816 -0.229952 -0.226367 -0.243335   \n70151        T4d       T4d       T4d       T4d       T4d       T4d       T4d   \n72259  -0.114836 -0.113646 -0.114635 -0.115028 -0.114903 -0.114248 -0.114154   \n72260        T5b       T5b       T5b       T5b       T5b       T5b       T5b   \n79206  -0.087025 -0.086356 -0.085947 -0.084947 -0.086957 -0.083834 -0.086843   \n79207       Tm5Y      Tm5Y      Tm5Y      Tm5Y      Tm5Y      Tm5Y      Tm5Y   \n80237  -0.213761 -0.213547   -0.2134  -0.21188 -0.213546 -0.222629 -0.216487   \n80238         T1        T1        T1        T1        T1        T1        T1   \n103550  0.307632  0.309093  0.309326  0.308087  0.309148  0.302142  0.306854   \n103551        T3        T3        T3        T3        T3        T3        T3   \n114734  0.223833  0.232116  0.223903   0.22107  0.224855  0.224313  0.232702   \n114735       T2a       T2a       T2a       T2a       T2a       T2a       T2a   \n115847  0.453862  0.453889    0.4511  0.452336  0.445782  0.473783  0.452613   \n115848       Tm3       Tm3       Tm3       Tm3       Tm3       Tm3       Tm3   \n116443 -0.250846  -0.25081 -0.250459 -0.250231 -0.250443 -0.204988 -0.250757   \n116444      Tm16      Tm16      Tm16      Tm16      Tm16      Tm16      Tm16   \n117508 -0.165047 -0.162252 -0.164362  -0.15417  -0.16391  -0.16542 -0.164838   \n117509      TmY4      TmY4      TmY4      TmY4      TmY4      TmY4      TmY4   \n118855 -0.188718  -0.18836 -0.190188 -0.191399 -0.191628 -0.187714 -0.190163   \n118856       T5c       T5c       T5c       T5c       T5c       T5c       T5c   \n126700 -0.168271  -0.16845 -0.170567 -0.174824 -0.169887 -0.168659 -0.171407   \n126701       T5d       T5d       T5d       T5d       T5d       T5d       T5d   \n130001 -0.216106 -0.217303 -0.217275   -0.2146 -0.225504 -0.217264 -0.215955   \n130002      Tm5a      Tm5a      Tm5a      Tm5a      Tm5a      Tm5a      Tm5a   \n134939  0.054328  0.046511  0.034667  0.042274  0.057243  0.049949  0.051994   \n134940       T4a       T4a       T4a       T4a       T4a       T4a       T4a   \n138452  0.361677  0.352439  0.363724  0.363518  0.361208  0.361007  0.357717   \n138453      Tm5c      Tm5c      Tm5c      Tm5c      Tm5c      Tm5c      Tm5c   \n138640 -0.117619 -0.116515 -0.117921 -0.119411 -0.117762 -0.118086 -0.120435   \n138641       T5a       T5a       T5a       T5a       T5a       T5a       T5a   \n\n              99  \n294    -0.274867  \n295          Tm2  \n16391   0.289312  \n16392      TmY15  \n35210  -0.537031  \n35211       TmY3  \n36116   0.446533  \n36117      TmY18  \n41865  -0.032693  \n41866        T4c  \n42974   1.610999  \n42975         T2  \n43964   0.006728  \n43965        Tm1  \n52870  -0.453276  \n52871      TmY14  \n53868  -0.240473  \n53869        Tm9  \n54704   0.017329  \n54705        T4b  \n54826  -0.161465  \n54827        Tm4  \n59553   0.268421  \n59554       TmY9  \n62921   0.017321  \n62922      TmY5a  \n65086  -0.010855  \n65087       Tm20  \n70150   -0.22944  \n70151        T4d  \n72259  -0.113792  \n72260        T5b  \n79206  -0.084581  \n79207       Tm5Y  \n80237   -0.21357  \n80238         T1  \n103550  0.308255  \n103551        T3  \n114734  0.222674  \n114735       T2a  \n115847  0.445277  \n115848       Tm3  \n116443   -0.2504  \n116444      Tm16  \n117508 -0.142507  \n117509      TmY4  \n118855  -0.19185  \n118856       T5c  \n126700 -0.172184  \n126701       T5d  \n130001 -0.216403  \n130002      Tm5a  \n134939  0.048716  \n134940       T4a  \n138452  0.360965  \n138453      Tm5c  \n138640 -0.117311  \n138641       T5a  \n\n[58 rows x 102 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>root_id</th>\n      <th>cell_type</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>294</th>\n      <td>720575940626442596</td>\n      <td>Tm2</td>\n      <td>-0.274404</td>\n      <td>-0.269373</td>\n      <td>-0.277603</td>\n      <td>-0.268852</td>\n      <td>-0.275545</td>\n      <td>-0.273812</td>\n      <td>-0.276923</td>\n      <td>-0.274063</td>\n      <td>...</td>\n      <td>-0.281386</td>\n      <td>-0.273345</td>\n      <td>-0.274692</td>\n      <td>-0.284599</td>\n      <td>-0.275491</td>\n      <td>-0.276765</td>\n      <td>-0.283328</td>\n      <td>-0.289558</td>\n      <td>-0.274623</td>\n      <td>-0.274867</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>720575940626442596</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>...</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n      <td>Tm2</td>\n    </tr>\n    <tr>\n      <th>16391</th>\n      <td>720575940631359412</td>\n      <td>TmY15</td>\n      <td>0.290887</td>\n      <td>0.30118</td>\n      <td>0.309714</td>\n      <td>0.293187</td>\n      <td>0.284795</td>\n      <td>0.290941</td>\n      <td>0.275176</td>\n      <td>0.291664</td>\n      <td>...</td>\n      <td>0.292876</td>\n      <td>0.296338</td>\n      <td>0.29092</td>\n      <td>0.295088</td>\n      <td>0.286905</td>\n      <td>0.290851</td>\n      <td>0.295875</td>\n      <td>0.296853</td>\n      <td>0.303404</td>\n      <td>0.289312</td>\n    </tr>\n    <tr>\n      <th>16392</th>\n      <td>720575940631359412</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>...</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n      <td>TmY15</td>\n    </tr>\n    <tr>\n      <th>35210</th>\n      <td>720575940622440572</td>\n      <td>TmY3</td>\n      <td>-0.536687</td>\n      <td>-0.540938</td>\n      <td>-0.505989</td>\n      <td>-0.538004</td>\n      <td>-0.53686</td>\n      <td>-0.544931</td>\n      <td>-0.565986</td>\n      <td>-0.537135</td>\n      <td>...</td>\n      <td>-0.543615</td>\n      <td>-0.531848</td>\n      <td>-0.536896</td>\n      <td>-0.528557</td>\n      <td>-0.541048</td>\n      <td>-0.538066</td>\n      <td>-0.536549</td>\n      <td>-0.533267</td>\n      <td>-0.535469</td>\n      <td>-0.537031</td>\n    </tr>\n    <tr>\n      <th>35211</th>\n      <td>720575940622440572</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>...</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n      <td>TmY3</td>\n    </tr>\n    <tr>\n      <th>36116</th>\n      <td>720575940627533992</td>\n      <td>TmY18</td>\n      <td>0.455759</td>\n      <td>0.444986</td>\n      <td>0.456813</td>\n      <td>0.422267</td>\n      <td>0.45365</td>\n      <td>0.456177</td>\n      <td>0.452744</td>\n      <td>0.453379</td>\n      <td>...</td>\n      <td>0.435793</td>\n      <td>0.454588</td>\n      <td>0.452333</td>\n      <td>0.452292</td>\n      <td>0.447814</td>\n      <td>0.426964</td>\n      <td>0.448956</td>\n      <td>0.46067</td>\n      <td>0.45729</td>\n      <td>0.446533</td>\n    </tr>\n    <tr>\n      <th>36117</th>\n      <td>720575940627533992</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>...</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n      <td>TmY18</td>\n    </tr>\n    <tr>\n      <th>41865</th>\n      <td>720575940627442831</td>\n      <td>T4c</td>\n      <td>-0.027249</td>\n      <td>-0.027896</td>\n      <td>-0.025447</td>\n      <td>-0.036456</td>\n      <td>-0.022821</td>\n      <td>-0.021151</td>\n      <td>-0.033814</td>\n      <td>-0.027082</td>\n      <td>...</td>\n      <td>-0.033502</td>\n      <td>-0.015781</td>\n      <td>-0.027234</td>\n      <td>-0.009681</td>\n      <td>-0.017577</td>\n      <td>-0.02853</td>\n      <td>-0.022715</td>\n      <td>-0.032866</td>\n      <td>-0.021058</td>\n      <td>-0.032693</td>\n    </tr>\n    <tr>\n      <th>41866</th>\n      <td>720575940627442831</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>...</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n      <td>T4c</td>\n    </tr>\n    <tr>\n      <th>42974</th>\n      <td>720575940613432547</td>\n      <td>T2</td>\n      <td>1.643621</td>\n      <td>1.634292</td>\n      <td>1.664171</td>\n      <td>1.644273</td>\n      <td>1.632857</td>\n      <td>1.648688</td>\n      <td>1.63838</td>\n      <td>1.645779</td>\n      <td>...</td>\n      <td>1.617326</td>\n      <td>1.632607</td>\n      <td>1.647115</td>\n      <td>1.637437</td>\n      <td>1.643038</td>\n      <td>1.637348</td>\n      <td>1.652678</td>\n      <td>1.624251</td>\n      <td>1.64793</td>\n      <td>1.610999</td>\n    </tr>\n    <tr>\n      <th>42975</th>\n      <td>720575940613432547</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>...</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n      <td>T2</td>\n    </tr>\n    <tr>\n      <th>43964</th>\n      <td>720575940612429938</td>\n      <td>Tm1</td>\n      <td>0.006562</td>\n      <td>0.00664</td>\n      <td>0.006699</td>\n      <td>0.0066</td>\n      <td>0.007688</td>\n      <td>0.00672</td>\n      <td>0.005995</td>\n      <td>0.006526</td>\n      <td>...</td>\n      <td>0.005806</td>\n      <td>0.007109</td>\n      <td>0.00668</td>\n      <td>0.006631</td>\n      <td>0.006792</td>\n      <td>0.006399</td>\n      <td>0.00666</td>\n      <td>0.006982</td>\n      <td>0.006732</td>\n      <td>0.006728</td>\n    </tr>\n    <tr>\n      <th>43965</th>\n      <td>720575940612429938</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>...</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n      <td>Tm1</td>\n    </tr>\n    <tr>\n      <th>52870</th>\n      <td>720575940615538847</td>\n      <td>TmY14</td>\n      <td>-0.470678</td>\n      <td>-0.485449</td>\n      <td>-0.486087</td>\n      <td>-0.487877</td>\n      <td>-0.463684</td>\n      <td>-0.487647</td>\n      <td>-0.491936</td>\n      <td>-0.486837</td>\n      <td>...</td>\n      <td>-0.488204</td>\n      <td>-0.480616</td>\n      <td>-0.486542</td>\n      <td>-0.483265</td>\n      <td>-0.485588</td>\n      <td>-0.486988</td>\n      <td>-0.478718</td>\n      <td>-0.486942</td>\n      <td>-0.479125</td>\n      <td>-0.453276</td>\n    </tr>\n    <tr>\n      <th>52871</th>\n      <td>720575940615538847</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>...</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n      <td>TmY14</td>\n    </tr>\n    <tr>\n      <th>53868</th>\n      <td>720575940631272908</td>\n      <td>Tm9</td>\n      <td>-0.230167</td>\n      <td>-0.236416</td>\n      <td>-0.236989</td>\n      <td>-0.228967</td>\n      <td>-0.230719</td>\n      <td>-0.245778</td>\n      <td>-0.235024</td>\n      <td>-0.25435</td>\n      <td>...</td>\n      <td>-0.219087</td>\n      <td>-0.236078</td>\n      <td>-0.236573</td>\n      <td>-0.243764</td>\n      <td>-0.233919</td>\n      <td>-0.23549</td>\n      <td>-0.225156</td>\n      <td>-0.235632</td>\n      <td>-0.242537</td>\n      <td>-0.240473</td>\n    </tr>\n    <tr>\n      <th>53869</th>\n      <td>720575940631272908</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>...</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n      <td>Tm9</td>\n    </tr>\n    <tr>\n      <th>54704</th>\n      <td>720575940623153961</td>\n      <td>T4b</td>\n      <td>0.027528</td>\n      <td>0.02611</td>\n      <td>0.0311</td>\n      <td>0.027853</td>\n      <td>0.021825</td>\n      <td>0.017837</td>\n      <td>0.030688</td>\n      <td>0.02708</td>\n      <td>...</td>\n      <td>0.021439</td>\n      <td>0.024752</td>\n      <td>0.025348</td>\n      <td>0.025486</td>\n      <td>0.048752</td>\n      <td>0.047477</td>\n      <td>0.031102</td>\n      <td>0.035537</td>\n      <td>0.033298</td>\n      <td>0.017329</td>\n    </tr>\n    <tr>\n      <th>54705</th>\n      <td>720575940623153961</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>...</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n      <td>T4b</td>\n    </tr>\n    <tr>\n      <th>54826</th>\n      <td>720575940629147445</td>\n      <td>Tm4</td>\n      <td>-0.167922</td>\n      <td>-0.163578</td>\n      <td>-0.164672</td>\n      <td>-0.165606</td>\n      <td>-0.161746</td>\n      <td>-0.169276</td>\n      <td>-0.164354</td>\n      <td>-0.163264</td>\n      <td>...</td>\n      <td>-0.163247</td>\n      <td>-0.164718</td>\n      <td>-0.164774</td>\n      <td>-0.164814</td>\n      <td>-0.167767</td>\n      <td>-0.165545</td>\n      <td>-0.165277</td>\n      <td>-0.166722</td>\n      <td>-0.165834</td>\n      <td>-0.161465</td>\n    </tr>\n    <tr>\n      <th>54827</th>\n      <td>720575940629147445</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>...</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n      <td>Tm4</td>\n    </tr>\n    <tr>\n      <th>59553</th>\n      <td>720575940615205031</td>\n      <td>TmY9</td>\n      <td>0.270779</td>\n      <td>0.270226</td>\n      <td>0.267313</td>\n      <td>0.267039</td>\n      <td>0.268321</td>\n      <td>0.268815</td>\n      <td>0.267335</td>\n      <td>0.27023</td>\n      <td>...</td>\n      <td>0.265036</td>\n      <td>0.269848</td>\n      <td>0.2737</td>\n      <td>0.269777</td>\n      <td>0.276135</td>\n      <td>0.267619</td>\n      <td>0.271146</td>\n      <td>0.270308</td>\n      <td>0.273503</td>\n      <td>0.268421</td>\n    </tr>\n    <tr>\n      <th>59554</th>\n      <td>720575940615205031</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>...</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n      <td>TmY9</td>\n    </tr>\n    <tr>\n      <th>62921</th>\n      <td>720575940644202696</td>\n      <td>TmY5a</td>\n      <td>0.024819</td>\n      <td>0.019574</td>\n      <td>0.021826</td>\n      <td>0.02261</td>\n      <td>0.022273</td>\n      <td>0.022178</td>\n      <td>0.019672</td>\n      <td>0.022927</td>\n      <td>...</td>\n      <td>0.025591</td>\n      <td>0.013931</td>\n      <td>0.021971</td>\n      <td>0.020398</td>\n      <td>0.018514</td>\n      <td>0.021708</td>\n      <td>0.024555</td>\n      <td>0.015851</td>\n      <td>0.023967</td>\n      <td>0.017321</td>\n    </tr>\n    <tr>\n      <th>62922</th>\n      <td>720575940644202696</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>...</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n      <td>TmY5a</td>\n    </tr>\n    <tr>\n      <th>65086</th>\n      <td>720575940633128751</td>\n      <td>Tm20</td>\n      <td>-0.012243</td>\n      <td>-0.011983</td>\n      <td>-0.011619</td>\n      <td>-0.011609</td>\n      <td>-0.010821</td>\n      <td>-0.011556</td>\n      <td>-0.012491</td>\n      <td>-0.011911</td>\n      <td>...</td>\n      <td>-0.011725</td>\n      <td>-0.011131</td>\n      <td>-0.011791</td>\n      <td>-0.012418</td>\n      <td>-0.011989</td>\n      <td>-0.012521</td>\n      <td>-0.012037</td>\n      <td>-0.012996</td>\n      <td>-0.012268</td>\n      <td>-0.010855</td>\n    </tr>\n    <tr>\n      <th>65087</th>\n      <td>720575940633128751</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>...</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n      <td>Tm20</td>\n    </tr>\n    <tr>\n      <th>70150</th>\n      <td>720575940639020351</td>\n      <td>T4d</td>\n      <td>-0.228009</td>\n      <td>-0.212095</td>\n      <td>-0.246063</td>\n      <td>-0.228065</td>\n      <td>-0.233154</td>\n      <td>-0.233806</td>\n      <td>-0.224727</td>\n      <td>-0.215198</td>\n      <td>...</td>\n      <td>-0.244032</td>\n      <td>-0.228817</td>\n      <td>-0.228752</td>\n      <td>-0.228597</td>\n      <td>-0.230503</td>\n      <td>-0.210816</td>\n      <td>-0.229952</td>\n      <td>-0.226367</td>\n      <td>-0.243335</td>\n      <td>-0.22944</td>\n    </tr>\n    <tr>\n      <th>70151</th>\n      <td>720575940639020351</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>...</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n      <td>T4d</td>\n    </tr>\n    <tr>\n      <th>72259</th>\n      <td>720575940628618106</td>\n      <td>T5b</td>\n      <td>-0.113973</td>\n      <td>-0.114224</td>\n      <td>-0.114975</td>\n      <td>-0.116502</td>\n      <td>-0.114336</td>\n      <td>-0.112988</td>\n      <td>-0.113027</td>\n      <td>-0.116325</td>\n      <td>...</td>\n      <td>-0.113512</td>\n      <td>-0.114633</td>\n      <td>-0.114836</td>\n      <td>-0.113646</td>\n      <td>-0.114635</td>\n      <td>-0.115028</td>\n      <td>-0.114903</td>\n      <td>-0.114248</td>\n      <td>-0.114154</td>\n      <td>-0.113792</td>\n    </tr>\n    <tr>\n      <th>72260</th>\n      <td>720575940628618106</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>...</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n      <td>T5b</td>\n    </tr>\n    <tr>\n      <th>79206</th>\n      <td>720575940629907906</td>\n      <td>Tm5Y</td>\n      <td>-0.08547</td>\n      <td>-0.086101</td>\n      <td>-0.087118</td>\n      <td>-0.084572</td>\n      <td>-0.086759</td>\n      <td>-0.084696</td>\n      <td>-0.086643</td>\n      <td>-0.085479</td>\n      <td>...</td>\n      <td>-0.084885</td>\n      <td>-0.084173</td>\n      <td>-0.087025</td>\n      <td>-0.086356</td>\n      <td>-0.085947</td>\n      <td>-0.084947</td>\n      <td>-0.086957</td>\n      <td>-0.083834</td>\n      <td>-0.086843</td>\n      <td>-0.084581</td>\n    </tr>\n    <tr>\n      <th>79207</th>\n      <td>720575940629907906</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>...</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n      <td>Tm5Y</td>\n    </tr>\n    <tr>\n      <th>80237</th>\n      <td>720575940614015981</td>\n      <td>T1</td>\n      <td>-0.213574</td>\n      <td>-0.213596</td>\n      <td>-0.208285</td>\n      <td>-0.209546</td>\n      <td>-0.212187</td>\n      <td>-0.206088</td>\n      <td>-0.221792</td>\n      <td>-0.213497</td>\n      <td>...</td>\n      <td>-0.209885</td>\n      <td>-0.209927</td>\n      <td>-0.213761</td>\n      <td>-0.213547</td>\n      <td>-0.2134</td>\n      <td>-0.21188</td>\n      <td>-0.213546</td>\n      <td>-0.222629</td>\n      <td>-0.216487</td>\n      <td>-0.21357</td>\n    </tr>\n    <tr>\n      <th>80238</th>\n      <td>720575940614015981</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>...</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n      <td>T1</td>\n    </tr>\n    <tr>\n      <th>103550</th>\n      <td>720575940614218407</td>\n      <td>T3</td>\n      <td>0.309847</td>\n      <td>0.302557</td>\n      <td>0.300639</td>\n      <td>0.306294</td>\n      <td>0.301769</td>\n      <td>0.307941</td>\n      <td>0.315403</td>\n      <td>0.311763</td>\n      <td>...</td>\n      <td>0.311101</td>\n      <td>0.294312</td>\n      <td>0.307632</td>\n      <td>0.309093</td>\n      <td>0.309326</td>\n      <td>0.308087</td>\n      <td>0.309148</td>\n      <td>0.302142</td>\n      <td>0.306854</td>\n      <td>0.308255</td>\n    </tr>\n    <tr>\n      <th>103551</th>\n      <td>720575940614218407</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>...</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n      <td>T3</td>\n    </tr>\n    <tr>\n      <th>114734</th>\n      <td>720575940625277710</td>\n      <td>T2a</td>\n      <td>0.239278</td>\n      <td>0.224761</td>\n      <td>0.224668</td>\n      <td>0.218674</td>\n      <td>0.216012</td>\n      <td>0.225532</td>\n      <td>0.218613</td>\n      <td>0.222221</td>\n      <td>...</td>\n      <td>0.230426</td>\n      <td>0.223746</td>\n      <td>0.223833</td>\n      <td>0.232116</td>\n      <td>0.223903</td>\n      <td>0.22107</td>\n      <td>0.224855</td>\n      <td>0.224313</td>\n      <td>0.232702</td>\n      <td>0.222674</td>\n    </tr>\n    <tr>\n      <th>114735</th>\n      <td>720575940625277710</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>...</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n      <td>T2a</td>\n    </tr>\n    <tr>\n      <th>115847</th>\n      <td>720575940612468965</td>\n      <td>Tm3</td>\n      <td>0.440971</td>\n      <td>0.442472</td>\n      <td>0.426341</td>\n      <td>0.441889</td>\n      <td>0.439991</td>\n      <td>0.436614</td>\n      <td>0.438033</td>\n      <td>0.443253</td>\n      <td>...</td>\n      <td>0.435641</td>\n      <td>0.425332</td>\n      <td>0.453862</td>\n      <td>0.453889</td>\n      <td>0.4511</td>\n      <td>0.452336</td>\n      <td>0.445782</td>\n      <td>0.473783</td>\n      <td>0.452613</td>\n      <td>0.445277</td>\n    </tr>\n    <tr>\n      <th>115848</th>\n      <td>720575940612468965</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>...</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n      <td>Tm3</td>\n    </tr>\n    <tr>\n      <th>116443</th>\n      <td>720575940615435926</td>\n      <td>Tm16</td>\n      <td>-0.250778</td>\n      <td>-0.250813</td>\n      <td>-0.250809</td>\n      <td>-0.250606</td>\n      <td>-0.273291</td>\n      <td>-0.251688</td>\n      <td>-0.250911</td>\n      <td>-0.250795</td>\n      <td>...</td>\n      <td>-0.229925</td>\n      <td>-0.202998</td>\n      <td>-0.250846</td>\n      <td>-0.25081</td>\n      <td>-0.250459</td>\n      <td>-0.250231</td>\n      <td>-0.250443</td>\n      <td>-0.204988</td>\n      <td>-0.250757</td>\n      <td>-0.2504</td>\n    </tr>\n    <tr>\n      <th>116444</th>\n      <td>720575940615435926</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>...</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n      <td>Tm16</td>\n    </tr>\n    <tr>\n      <th>117508</th>\n      <td>720575940618950401</td>\n      <td>TmY4</td>\n      <td>-0.165038</td>\n      <td>-0.164958</td>\n      <td>-0.172305</td>\n      <td>-0.164963</td>\n      <td>-0.164548</td>\n      <td>-0.165027</td>\n      <td>-0.166158</td>\n      <td>-0.165094</td>\n      <td>...</td>\n      <td>-0.164789</td>\n      <td>-0.165115</td>\n      <td>-0.165047</td>\n      <td>-0.162252</td>\n      <td>-0.164362</td>\n      <td>-0.15417</td>\n      <td>-0.16391</td>\n      <td>-0.16542</td>\n      <td>-0.164838</td>\n      <td>-0.142507</td>\n    </tr>\n    <tr>\n      <th>117509</th>\n      <td>720575940618950401</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>...</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n      <td>TmY4</td>\n    </tr>\n    <tr>\n      <th>118855</th>\n      <td>720575940630995983</td>\n      <td>T5c</td>\n      <td>-0.191226</td>\n      <td>-0.186858</td>\n      <td>-0.19287</td>\n      <td>-0.189666</td>\n      <td>-0.197243</td>\n      <td>-0.190713</td>\n      <td>-0.19199</td>\n      <td>-0.189586</td>\n      <td>...</td>\n      <td>-0.191443</td>\n      <td>-0.187489</td>\n      <td>-0.188718</td>\n      <td>-0.18836</td>\n      <td>-0.190188</td>\n      <td>-0.191399</td>\n      <td>-0.191628</td>\n      <td>-0.187714</td>\n      <td>-0.190163</td>\n      <td>-0.19185</td>\n    </tr>\n    <tr>\n      <th>118856</th>\n      <td>720575940630995983</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>...</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n      <td>T5c</td>\n    </tr>\n    <tr>\n      <th>126700</th>\n      <td>720575940620108411</td>\n      <td>T5d</td>\n      <td>-0.168876</td>\n      <td>-0.167089</td>\n      <td>-0.168554</td>\n      <td>-0.169222</td>\n      <td>-0.171482</td>\n      <td>-0.166379</td>\n      <td>-0.168502</td>\n      <td>-0.170454</td>\n      <td>...</td>\n      <td>-0.16047</td>\n      <td>-0.165863</td>\n      <td>-0.168271</td>\n      <td>-0.16845</td>\n      <td>-0.170567</td>\n      <td>-0.174824</td>\n      <td>-0.169887</td>\n      <td>-0.168659</td>\n      <td>-0.171407</td>\n      <td>-0.172184</td>\n    </tr>\n    <tr>\n      <th>126701</th>\n      <td>720575940620108411</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>...</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n      <td>T5d</td>\n    </tr>\n    <tr>\n      <th>130001</th>\n      <td>720575940636345967</td>\n      <td>Tm5a</td>\n      <td>-0.248215</td>\n      <td>-0.217244</td>\n      <td>-0.217475</td>\n      <td>-0.217276</td>\n      <td>-0.216703</td>\n      <td>-0.217197</td>\n      <td>-0.217233</td>\n      <td>-0.217113</td>\n      <td>...</td>\n      <td>-0.217554</td>\n      <td>-0.217277</td>\n      <td>-0.216106</td>\n      <td>-0.217303</td>\n      <td>-0.217275</td>\n      <td>-0.2146</td>\n      <td>-0.225504</td>\n      <td>-0.217264</td>\n      <td>-0.215955</td>\n      <td>-0.216403</td>\n    </tr>\n    <tr>\n      <th>130002</th>\n      <td>720575940636345967</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>...</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n      <td>Tm5a</td>\n    </tr>\n    <tr>\n      <th>134939</th>\n      <td>720575940618936733</td>\n      <td>T4a</td>\n      <td>0.041009</td>\n      <td>0.043589</td>\n      <td>0.046163</td>\n      <td>0.048303</td>\n      <td>0.046084</td>\n      <td>0.041624</td>\n      <td>0.047951</td>\n      <td>0.038744</td>\n      <td>...</td>\n      <td>0.048636</td>\n      <td>0.03479</td>\n      <td>0.054328</td>\n      <td>0.046511</td>\n      <td>0.034667</td>\n      <td>0.042274</td>\n      <td>0.057243</td>\n      <td>0.049949</td>\n      <td>0.051994</td>\n      <td>0.048716</td>\n    </tr>\n    <tr>\n      <th>134940</th>\n      <td>720575940618936733</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>...</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n      <td>T4a</td>\n    </tr>\n    <tr>\n      <th>138452</th>\n      <td>720575940610765124</td>\n      <td>Tm5c</td>\n      <td>0.361797</td>\n      <td>0.360946</td>\n      <td>0.358235</td>\n      <td>0.373954</td>\n      <td>0.368736</td>\n      <td>0.353623</td>\n      <td>0.329776</td>\n      <td>0.364594</td>\n      <td>...</td>\n      <td>0.350202</td>\n      <td>0.345176</td>\n      <td>0.361677</td>\n      <td>0.352439</td>\n      <td>0.363724</td>\n      <td>0.363518</td>\n      <td>0.361208</td>\n      <td>0.361007</td>\n      <td>0.357717</td>\n      <td>0.360965</td>\n    </tr>\n    <tr>\n      <th>138453</th>\n      <td>720575940610765124</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>...</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n      <td>Tm5c</td>\n    </tr>\n    <tr>\n      <th>138640</th>\n      <td>720575940621298949</td>\n      <td>T5a</td>\n      <td>-0.116278</td>\n      <td>-0.116406</td>\n      <td>-0.120942</td>\n      <td>-0.110424</td>\n      <td>-0.11729</td>\n      <td>-0.118717</td>\n      <td>-0.1174</td>\n      <td>-0.119601</td>\n      <td>...</td>\n      <td>-0.11125</td>\n      <td>-0.121155</td>\n      <td>-0.117619</td>\n      <td>-0.116515</td>\n      <td>-0.117921</td>\n      <td>-0.119411</td>\n      <td>-0.117762</td>\n      <td>-0.118086</td>\n      <td>-0.120435</td>\n      <td>-0.117311</td>\n    </tr>\n    <tr>\n      <th>138641</th>\n      <td>720575940621298949</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>...</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n      <td>T5a</td>\n    </tr>\n  </tbody>\n</table>\n<p>58 rows × 102 columns</p>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_root_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T11:14:53.522376719Z",
     "start_time": "2024-02-02T11:14:53.464191989Z"
    }
   },
   "id": "68b19cb656ac528b",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "27d60c774fb6d65c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
