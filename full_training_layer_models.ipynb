{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:40:02.816615Z",
     "start_time": "2024-03-28T14:39:58.709963Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import device, cuda, autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from random import sample\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "import flyvision\n",
    "from flyvision_ans import DECODING_CELLS\n",
    "from flyvision.utils.activity_utils import LayerActivity\n",
    "from from_image_to_video import image_paths_to_sequences\n",
    "from from_retina_to_connectome_funcs import from_retina_to_model, get_cell_type_indices, compute_voronoi_averages, from_retina_to_connectome\n",
    "from logs_to_wandb import log_images_to_wandb, log_running_stats_to_wandb\n",
    "from from_video_to_training_batched_funcs import get_files_from_directory, select_random_videos, paths_to_labels, \\\n",
    "    load_custom_sequences\n",
    "from from_retina_to_connectome_utils import hex_to_square_grid, initialize_results_df, predictions_and_corrects_from_model_results, update_results_df, update_running_loss, get_decision_making_neurons\n",
    "from adult_models import FullAdultModel\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='invalid value encountered in cast',\n",
    "    category=RuntimeWarning,\n",
    "    module='wandb.sdk.data_types.image'\n",
    ")\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "dtype = torch.float32\n",
    "\n",
    "device_type = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "device_type = \"cpu\"\n",
    "DEVICE = device(device_type)\n",
    "\n",
    "TRAINING_DATA_DIR = \"images/easy_v2\"\n",
    "TESTING_DATA_DIR = \"images/easy_images\"\n",
    "VALIDATION_DATA_DIR = \"images/easyval_images\"\n",
    "\n",
    "debugging = True\n",
    "debug_length = 20\n",
    "wandb_ = False\n",
    "wandb_images_every = 100\n",
    "small = True\n",
    "small_length = 2000\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "\n",
    "dropout = .1\n",
    "max_lr = 0.01\n",
    "base_lr = 0.001\n",
    "weight_decay = 0.0001\n",
    "NUM_CONNECTOME_PASSES=5\n",
    "\n",
    "use_one_cycle_lr = False\n",
    "\n",
    "model_config = {\n",
    "    \"debugging\": debugging,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"dropout\": dropout,\n",
    "    \"base_lr\": base_lr,\n",
    "    \"max_lr\": max_lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"num_connectome_passes\": NUM_CONNECTOME_PASSES,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b101e44d143f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:18:59.226154Z",
     "start_time": "2024-03-28T14:18:54.651619Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init stuff\n",
    "extent, kernel_size = 15, 13\n",
    "decision_making_vector = get_decision_making_neurons(dtype).to(DEVICE)\n",
    "receptors = flyvision.rendering.BoxEye(extent=extent, kernel_size=kernel_size)\n",
    "network_view = flyvision.NetworkView(flyvision.results_dir / \"opticflow/000/0000\")\n",
    "network = network_view.init_network(chkpt=\"best_chkpt\")\n",
    "classification = pd.read_csv(\"adult_data/classification_clean.csv\")\n",
    "root_id_to_index = pd.read_csv(\"adult_data/root_id_to_index.csv\")\n",
    "dt = 1 / 100 # some parameter from flyvision\n",
    "last_good_frame = 2\n",
    "cell_type_plot = \"TmY18\"\n",
    "\n",
    "cell_type_indices = get_cell_type_indices(classification, root_id_to_index, DECODING_CELLS)\n",
    "\n",
    "training_videos = get_files_from_directory(TRAINING_DATA_DIR)\n",
    "test_videos = get_files_from_directory(TESTING_DATA_DIR)\n",
    "validation_videos = get_files_from_directory(TESTING_DATA_DIR)\n",
    "\n",
    "if small:\n",
    "    training_videos = sample(training_videos, small_length)\n",
    "    test_videos = sample(test_videos, small_length)\n",
    "    validation_videos = sample(validation_videos, int(small_length / 5))\n",
    "\n",
    "if len(training_videos) == 0:\n",
    "    print(\"I can't find any training images or videos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fce80a3502cf3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:18:59.238988Z",
     "start_time": "2024-03-28T14:18:59.227248Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eudald/Desktop/doctorat/connectome/adult_models.py:55: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  torch.sparse_csr_tensor(\n"
     ]
    }
   ],
   "source": [
    "synaptic_matrix = load_npz(\"adult_data/synaptic_matrix_sparse.npz\").tocsr()\n",
    "\n",
    "model = FullAdultModel(\n",
    "    synaptic_matrix,\n",
    "    decision_making_vector,\n",
    "    root_id_to_index.shape[0],\n",
    "    NUM_CONNECTOME_PASSES,\n",
    "    batch_size=batch_size,\n",
    "    dtype=dtype,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fa2f7661646a93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:22:54.362080Z",
     "start_time": "2024-03-28T14:19:58.045941Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unsupported memory format Preserve",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, labels)\n\u001b[1;32m     73\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 74\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Calculate run parameters\u001b[39;00m\n\u001b[1;32m     77\u001b[0m predictions, labels_cpu, correct \u001b[38;5;241m=\u001b[39m predictions_and_corrects_from_model_results(out, labels)\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/optim/adam.py:154\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/optim/adam.py:109\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    103\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    104\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    111\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unsupported memory format Preserve"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "from from_retina_to_connectome_utils import create_csr_input\n",
    "\n",
    "\n",
    "if wandb_:\n",
    "    wandb.init(project=\"adult_connectome\", config=model_config)\n",
    "\n",
    "model.train()\n",
    "\n",
    "results = initialize_results_df()\n",
    "probabilities = []\n",
    "accuracies = []\n",
    "already_selected = []\n",
    "running_loss = 0.0\n",
    "total_correct = 0\n",
    "total = 0\n",
    "\n",
    "iterations = debug_length if debugging else len(training_videos) // batch_size\n",
    "\n",
    "for i in tqdm(range(iterations)):\n",
    "    batch_files, already_selected = select_random_videos(\n",
    "        training_videos, batch_size, already_selected\n",
    "    )\n",
    "    labels = paths_to_labels(batch_files)\n",
    "    batch_sequences = image_paths_to_sequences(batch_files)\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "\n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        # rendered sequences are in RGB; move it to 0-1 for better training\n",
    "        rendered_sequence = torch.div(rendered_sequence, 255)\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(\n",
    "            LayerActivity(simulation, network.connectome, keepref=True)\n",
    "        )\n",
    "\n",
    "    if wandb_ and i % wandb_images_every == 0:\n",
    "        la_0 = hex_to_square_grid(\n",
    "            layer_activations[0][cell_type_plot].squeeze()[-last_good_frame].cpu().numpy()\n",
    "            ),\n",
    "        log_images_to_wandb(batch_sequences[0], rendered_sequences[0], la_0, batch_files[0], frame=last_good_frame, cell_type=cell_type_plot)\n",
    "\n",
    "    voronoi_averages_df = compute_voronoi_averages(\n",
    "        layer_activations, classification, DECODING_CELLS, last_good_frame=last_good_frame\n",
    "    )\n",
    "    # normalize column wise (except last column) using apply\n",
    "    values_cols = voronoi_averages_df.columns != \"index_name\"\n",
    "    voronoi_averages_df.loc[:, values_cols] = voronoi_averages_df.loc[:, values_cols].apply(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()), axis=0\n",
    "        )\n",
    "\n",
    "    activation_df = from_retina_to_connectome(\n",
    "        voronoi_averages_df, classification, root_id_to_index\n",
    "    )\n",
    "    del layer_activations, rendered_sequences, rendered_sequence, simulation\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if wandb_:\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=20)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # inputs = create_csr_input(activation_df)\n",
    "    inputs = torch.tensor(activation_df.values, dtype=dtype, device=DEVICE)\n",
    "    labels = torch.tensor(labels, dtype=dtype, device=DEVICE).unsqueeze(1)\n",
    "\n",
    "    out = model(inputs)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate run parameters\n",
    "    predictions, labels_cpu, correct = predictions_and_corrects_from_model_results(out, labels)\n",
    "    results = update_results_df(results, batch_files, predictions, labels_cpu, correct)\n",
    "    running_loss += update_running_loss(loss, inputs)\n",
    "    total += labels.shape[0]\n",
    "    total_correct += correct.sum()\n",
    "\n",
    "    if wandb_:\n",
    "        log_running_stats_to_wandb(0, i, running_loss, total_correct, total, results)\n",
    "\n",
    "print(f\"Finished training with loss {running_loss / total} and accuracy {total_correct / total}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33a081fe1b9a883e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T09:37:12.123162Z",
     "start_time": "2024-03-28T09:37:11.886793Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1960 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m batch_sequences \u001b[38;5;241m=\u001b[39m load_custom_sequences(batch_files)  \u001b[38;5;66;03m# Load and preprocess the video sequences\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming receptors is a function that processes your sequences\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m rendered_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mreceptors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m layer_activations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rendered_sequence \u001b[38;5;129;01min\u001b[39;00m rendered_sequences:\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/flyvision/rendering/eye.py:116\u001b[0m, in \u001b[0;36mBoxEye.__call__\u001b[0;34m(self, sequence, ftype, hex_sample)\u001b[0m\n\u001b[1;32m    111\u001b[0m samples, frames, height, width \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sequence, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mFloatTensor):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# auto-moving to GPU in case default tensor is cuda but passed\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# sequence is not for convenience\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_frame_size \u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([height, width]))\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# to rescale to the minimum frame size\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m ttf\u001b[38;5;241m.\u001b[39mresize(\n\u001b[1;32m    121\u001b[0m         sequence, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_frame_size\u001b[38;5;241m.\u001b[39mtolist(), antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "already_selected_validation = []\n",
    "# Assuming batch_size is defined\n",
    "for _ in tqdm(range(len(validation_videos) // batch_size)):\n",
    "    batch_files, already_selected_validation = select_random_videos(validation_videos, batch_size, already_selected_validation)\n",
    "\n",
    "    labels = paths_to_labels(batch_files)  # Convert paths to labels\n",
    "    batch_sequences = load_custom_sequences(batch_files)  # Load and preprocess the video sequences\n",
    "    \n",
    "    # Assuming receptors is a function that processes your sequences\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "    \n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(LayerActivity(simulation, network.connectome, keepref=True))\n",
    "        \n",
    "    del rendered_sequences, simulation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Preparing the data for the model, similar to training\n",
    "    inputs, labels = from_retina_to_model(layer_activations, labels, DECODING_CELLS, last_good_frame, classification, root_id_to_index)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).unsqueeze(-1).float()\n",
    "        \n",
    "        with autocast(device_type):\n",
    "            predictions = model(inputs)\n",
    "            # Assuming your criterion and evaluation metrics are defined similarly to training\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            # Calculate other metrics if necessary, e.g., accuracy\n",
    "            \n",
    "            # Log validation metrics to WandB\n",
    "            wandb.log({\"validation_loss\": loss.item()})\n",
    "            # Log other metrics similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93414a83f7bbd3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dabc4d2827d8cc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When cuda breaks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba39b3687e323bec",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for eudald: \n",
      "[sudo] password for eudald: \n"
     ]
    }
   ],
   "source": [
    "!sudo rmmod nvidia_uvm\n",
    "!sudo modprobe nvidia_uvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185e185",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
