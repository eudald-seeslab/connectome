{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T09:54:02.109022Z",
     "start_time": "2024-04-08T09:54:02.102030Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import device, cuda, autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import flyvision\n",
    "from flyvision.utils.activity_utils import LayerActivity\n",
    "from flyvision_ans import DECODING_CELLS\n",
    "import wandb\n",
    "from conv_models import DecodingImagesCNN\n",
    "from from_retina_to_connectome_utils import layer_activations_to_decoding_images, predictions_and_corrects_from_model_results, update_results_df, update_running_loss, initialize_results_df\n",
    "from from_video_to_training_batched_funcs import get_files_from_directory, select_random_videos, paths_to_labels\n",
    "from from_image_to_video import image_paths_to_sequences\n",
    "from utils import plot_weber_fraction\n",
    "from logs_to_wandb import log_running_stats_to_wandb, log_validation_stats_to_wandb\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='invalid value encountered in cast',\n",
    "    category=RuntimeWarning,\n",
    "    module='wandb.sdk.data_types.image'\n",
    ")\n",
    "\n",
    "device_type = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "DEVICE = device(device_type)\n",
    "torch.manual_seed(42)\n",
    "batch_size = 64\n",
    "last_good_frame = 2\n",
    "\n",
    "TRAINING_DATA_DIR = \"images/easy_v2\"\n",
    "TESTING_DATA_DIR = \"images/easyval_images\"\n",
    "\n",
    "debugging = True\n",
    "debug_length = 20\n",
    "wandb_ = False\n",
    "wandb_images_every = 10\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "channel_sizes = [16, 32, 64]\n",
    "mult_size = 1\n",
    "channel_sizes = [int(a * mult_size) for a in channel_sizes]\n",
    "dropout = .5\n",
    "lr = .005\n",
    "weight_decay = 5e-3\n",
    "\n",
    "model_config = {\n",
    "    \"debugging\": debugging,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"channel_sizes\": channel_sizes,\n",
    "    \"dropout\": dropout,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "1807fd157955627",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T09:54:07.904239Z",
     "start_time": "2024-04-08T09:54:02.752160Z"
    }
   },
   "source": [
    "network_view = flyvision.NetworkView(flyvision.results_dir / \"opticflow/000/0000\")\n",
    "network = network_view.init_network(chkpt=\"best_chkpt\")\n",
    "dt = 1 / 100\n",
    "extent, kernel_size = 15, 13\n",
    "receptors = flyvision.rendering.BoxEye(extent=extent, kernel_size=kernel_size)\n",
    "\n",
    "all_videos = get_files_from_directory(TRAINING_DATA_DIR)\n",
    "all_validation_videos = get_files_from_directory(TESTING_DATA_DIR)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "4fce80a3502cf3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T09:54:07.911809Z",
     "start_time": "2024-04-08T09:54:07.905460Z"
    }
   },
   "source": [
    "model = DecodingImagesCNN(\n",
    "    out_channels_1 = channel_sizes[0],\n",
    "    out_channels_2 = channel_sizes[1],\n",
    "    out_channels_3 = channel_sizes[2],\n",
    "    dropout = dropout\n",
    ").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = BCEWithLogitsLoss()"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "a3b94263294e5262",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T09:55:17.773987Z",
     "start_time": "2024-04-08T09:54:08.815939Z"
    }
   },
   "source": [
    "model.train()\n",
    "\n",
    "# Start wandb run\n",
    "if wandb_:\n",
    "    wandb.init(project='la_classification', config=model_config)\n",
    "\n",
    "results = initialize_results_df()\n",
    "\n",
    "# Training loop\n",
    "iterations = debug_length if debugging else len(all_videos) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    already_selected = []\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for i in tqdm(range(iterations)):\n",
    "        batch_files, already_selected = select_random_videos(\n",
    "            all_videos, batch_size, already_selected\n",
    "        )\n",
    "        labels = paths_to_labels(batch_files)\n",
    "        batch_sequences = image_paths_to_sequences(batch_files)\n",
    "        rendered_sequences = receptors(batch_sequences)\n",
    "        \n",
    "        layer_activations = []\n",
    "        for rendered_sequence in rendered_sequences:\n",
    "            # rendered sequences are in RGB; move it to 0-1 for better training\n",
    "            rendered_sequence = torch.div(rendered_sequence, 255)\n",
    "            # TODO: try to run this on cpu to parallelize it\n",
    "            simulation = network.simulate(rendered_sequence[None], dt)\n",
    "            layer_activations.append(\n",
    "                LayerActivity(simulation, network.connectome, keepref=True)\n",
    "            )\n",
    "        \n",
    "        decoding_images = layer_activations_to_decoding_images(layer_activations, last_good_frame, DECODING_CELLS)\n",
    "        \n",
    "        # Ensure the data is in tensor form and on the correct device\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float).to(DEVICE)\n",
    "        batch_sequences_tensor = torch.tensor(decoding_images, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "        # Create the dataset and dataloader\n",
    "        dataset = TensorDataset(batch_sequences_tensor, labels_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for inputs, batch_labels in dataloader:\n",
    "            inputs.to(DEVICE)\n",
    "            batch_labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            with autocast(device_type):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                # Calculate run parameters\n",
    "                predictions, batch_labels_cpu, correct = predictions_and_corrects_from_model_results(outputs, batch_labels)\n",
    "                results = update_results_df(results, batch_files, predictions, batch_labels_cpu, correct)\n",
    "                running_loss += update_running_loss(loss, inputs)\n",
    "                total += batch_labels.shape[0]\n",
    "                total_correct += correct.sum()\n",
    "                \n",
    "        # Log metrics to wandb\n",
    "        if wandb_:\n",
    "            log_running_stats_to_wandb(epoch, running_loss, total_correct, total, results)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / total}, Accuracy: {total_correct / total}')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [01:08<21:48, 68.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     29\u001B[0m     rendered_sequence \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdiv(rendered_sequence, \u001B[38;5;241m255\u001B[39m)\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;66;03m# TODO: try to run this on cpu to parallelize it\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m     simulation \u001B[38;5;241m=\u001B[39m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimulate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrendered_sequence\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m     layer_activations\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m     33\u001B[0m         LayerActivity(simulation, network\u001B[38;5;241m.\u001B[39mconnectome, keepref\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     34\u001B[0m     )\n\u001B[1;32m     36\u001B[0m decoding_images \u001B[38;5;241m=\u001B[39m layer_activations_to_decoding_images(layer_activations, last_good_frame, DECODING_CELLS)\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:498\u001B[0m, in \u001B[0;36mNetwork.simulate\u001B[0;34m(self, movie_input, dt, initial_state, as_states)\u001B[0m\n\u001B[1;32m    496\u001B[0m batch_size, n_frames \u001B[38;5;241m=\u001B[39m movie_input\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m initial_state \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 498\u001B[0m     initial_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msteady_state\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m simulation(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    500\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[1;32m    501\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m p\u001B[38;5;241m.\u001B[39mrequires_grad \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparameters()\n\u001B[1;32m    502\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:590\u001B[0m, in \u001B[0;36mNetwork.steady_state\u001B[0;34m(self, t_pre, dt, batch_size, value, state, grad, return_last)\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menable_grad(grad):\n\u001B[1;32m    589\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_last:\n\u001B[0;32m--> 590\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstimulus\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    591\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstimulus(), dt, as_states\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, state\u001B[38;5;241m=\u001B[39mstate)\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:545\u001B[0m, in \u001B[0;36mNetwork.forward\u001B[0;34m(self, x, dt, state, as_states)\u001B[0m\n\u001B[1;32m    542\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m state\n\u001B[1;32m    544\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m as_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 545\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    547\u001B[0m state_stack \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(\u001B[38;5;28mlist\u001B[39m(handle(state)), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    548\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m x, state\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:538\u001B[0m, in \u001B[0;36mNetwork.forward.<locals>.handle\u001B[0;34m(state)\u001B[0m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandle\u001B[39m(state):\n\u001B[1;32m    536\u001B[0m     \u001B[38;5;66;03m# loop over the temporal dimension for integration of dynamics\u001B[39;00m\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]):\n\u001B[0;32m--> 538\u001B[0m         state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    539\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m as_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    540\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m state\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mactivity\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/network.py:383\u001B[0m, in \u001B[0;36mNetwork._next_state\u001B[0;34m(self, params, state, x_t, dt)\u001B[0m\n\u001B[1;32m    368\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute the next state, given the current `state` and stimulus `x_t`.\u001B[39;00m\n\u001B[1;32m    369\u001B[0m \n\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;124;03mNote: simple, elementwise Euler integration.\u001B[39;00m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    381\u001B[0m vel \u001B[38;5;241m=\u001B[39m AutoDeref(nodes\u001B[38;5;241m=\u001B[39mAutoDeref(), edges\u001B[38;5;241m=\u001B[39mAutoDeref())\n\u001B[0;32m--> 383\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdynamics\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_state_velocity\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_sum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdt\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    387\u001B[0m next_state \u001B[38;5;241m=\u001B[39m AutoDeref(\n\u001B[1;32m    388\u001B[0m     nodes\u001B[38;5;241m=\u001B[39mAutoDeref(\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{k: state\u001B[38;5;241m.\u001B[39mnodes[k] \u001B[38;5;241m+\u001B[39m vel\u001B[38;5;241m.\u001B[39mnodes[k] \u001B[38;5;241m*\u001B[39m dt \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m state\u001B[38;5;241m.\u001B[39mnodes}\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    393\u001B[0m     ),\n\u001B[1;32m    394\u001B[0m )\n\u001B[1;32m    396\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_api(next_state)\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/flyvision/dynamics.py:172\u001B[0m, in \u001B[0;36mPPNeuronIGRSynapses.write_state_velocity\u001B[0;34m(self, vel, state, params, target_sum, x_t, dt, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrite_state_velocity\u001B[39m(\u001B[38;5;28mself\u001B[39m, vel, state, params, target_sum, x_t, dt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    169\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Velocity is the bias plus the sum of the weighted rectified inputs.\"\"\"\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     vel\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mactivity \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    171\u001B[0m         \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 172\u001B[0m         \u001B[38;5;241m/\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(params\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mtime_const, \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdt\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[1;32m    173\u001B[0m         \u001B[38;5;241m*\u001B[39m (\n\u001B[1;32m    174\u001B[0m             \u001B[38;5;241m-\u001B[39mstate\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mactivity\n\u001B[1;32m    175\u001B[0m             \u001B[38;5;241m+\u001B[39m params\u001B[38;5;241m.\u001B[39mnodes\u001B[38;5;241m.\u001B[39mbias\n\u001B[1;32m    176\u001B[0m             \u001B[38;5;241m+\u001B[39m target_sum(\n\u001B[1;32m    177\u001B[0m                 params\u001B[38;5;241m.\u001B[39medges\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(state\u001B[38;5;241m.\u001B[39msources\u001B[38;5;241m.\u001B[39mactivity)\n\u001B[1;32m    178\u001B[0m             )  \u001B[38;5;66;03m# internal chemical current\u001B[39;00m\n\u001B[1;32m    179\u001B[0m             \u001B[38;5;241m+\u001B[39m x_t\n\u001B[1;32m    180\u001B[0m         )\n\u001B[1;32m    181\u001B[0m     )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "15804021f0e43b93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T06:55:32.996401Z",
     "start_time": "2024-04-08T06:39:46.261719Z"
    }
   },
   "source": [
    "model.eval() \n",
    "\n",
    "total_correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "validation_results = initialize_results_df()\n",
    "\n",
    "with (torch.no_grad()):\n",
    "    for i in tqdm(range(len(all_validation_videos) // batch_size)):\n",
    "        batch_files, _ = select_random_videos(\n",
    "            all_validation_videos, batch_size, already_selected=[]\n",
    "        )\n",
    "        labels = paths_to_labels(batch_files)\n",
    "        batch_sequences = image_paths_to_sequences(batch_files)\n",
    "        rendered_sequences = receptors(batch_sequences)\n",
    "        \n",
    "        layer_activations = []\n",
    "        for rendered_sequence in rendered_sequences:\n",
    "            # rendered sequences are in RGB; move it to 0-1 for better training\n",
    "            rendered_sequence = torch.div(rendered_sequence, 255)\n",
    "            simulation = network.simulate(rendered_sequence[None], dt)\n",
    "            layer_activations.append(\n",
    "                LayerActivity(simulation, network.connectome, keepref=True)\n",
    "            )\n",
    "        \n",
    "        decoding_images = layer_activations_to_decoding_images(layer_activations, last_good_frame, DECODING_CELLS)\n",
    "        \n",
    "        # Ensure the data is in tensor form and on the correct device\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float).to(DEVICE)\n",
    "        batch_sequences_tensor = torch.tensor(decoding_images, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "        dataset = TensorDataset(batch_sequences_tensor, labels_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for inputs, batch_labels in dataloader:\n",
    "            inputs, batch_labels = inputs.to(DEVICE), batch_labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), batch_labels)\n",
    "\n",
    "            predictions, batch_labels_cpu, correct = predictions_and_corrects_from_model_results(outputs, batch_labels)\n",
    "            validation_results = update_results_df(validation_results, batch_files, predictions, batch_labels_cpu, correct)\n",
    "            running_loss += update_running_loss(loss, inputs)\n",
    "            total += batch_labels_cpu.shape[0]\n",
    "            total_correct += correct.sum().item()\n",
    "            \n",
    "            \n",
    "print(f'Validation Loss: {running_loss / total}, '\n",
    "      f'Validation Accuracy: {total_correct / total}')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [15:46<00:00, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.200814972158338, Validation Accuracy: 0.49001024590163933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "6a25342c29d5d247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T06:56:57.252140Z",
     "start_time": "2024-04-08T06:56:57.231623Z"
    }
   },
   "source": [
    "weber_plot = plot_weber_fraction(validation_results)\n",
    "\n",
    "# Log validation metrics to wandb\n",
    "if wandb_:\n",
    "    log_validation_stats_to_wandb(running_loss, total_correct, total, validation_results, weber_plot)\n",
    "    wandb.finish()"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m weber_plot \u001B[38;5;241m=\u001B[39m \u001B[43mplot_weber_fraction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_results\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Log validation metrics to wandb\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m wandb_:\n",
      "File \u001B[0;32m~/Desktop/doctorat/connectome/utils.py:41\u001B[0m, in \u001B[0;36mplot_weber_fraction\u001B[0;34m(results_df)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_weber_fraction\u001B[39m(results_df: pd\u001B[38;5;241m.\u001B[39mDataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m plt\u001B[38;5;241m.\u001B[39mFigure:\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;66;03m# Calculate the percentage of correct answers for each Weber ratio\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m     results_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myellow\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mresults_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mImage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m     42\u001B[0m     results_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblue\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m results_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m3\u001B[39m])\n\u001B[1;32m     43\u001B[0m     results_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweber_ratio\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m results_df\u001B[38;5;241m.\u001B[39mapply(\n\u001B[1;32m     44\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m row: \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myellow\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblue\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n\u001B[1;32m     45\u001B[0m         \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myellow\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mint\u001B[39m(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblue\u001B[39m\u001B[38;5;124m\"\u001B[39m])),\n\u001B[1;32m     46\u001B[0m         axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     47\u001B[0m     )\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T06:57:46.565470Z",
     "start_time": "2024-04-08T06:57:46.561960Z"
    }
   },
   "cell_type": "code",
   "source": "validation_results",
   "id": "7fa3503bd9d894eb",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd691e219cd4632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:25:28.370784Z",
     "start_time": "2024-04-03T13:25:28.343060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "          Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "              Conv2d-1     [1, 34, 31, 31]           4,912           4,912\n",
      "         BatchNorm2d-2     [1, 16, 31, 31]              32              32\n",
      "             Dropout-3     [1, 16, 15, 15]               0               0\n",
      "              Conv2d-4     [1, 16, 15, 15]           4,640           4,640\n",
      "         BatchNorm2d-5     [1, 32, 15, 15]              64              64\n",
      "              Conv2d-6       [1, 32, 7, 7]          18,496          18,496\n",
      "         BatchNorm2d-7       [1, 64, 7, 7]             128             128\n",
      "   AdaptiveAvgPool2d-8       [1, 64, 3, 3]               0               0\n",
      "              Linear-9             [1, 64]              65              65\n",
      "===========================================================================\n",
      "Total params: 28,337\n",
      "Trainable params: 28,337\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pytorch_model_summary import summary\n",
    "\n",
    "print(summary(DecodingImagesCNN(), torch.zeros((1, 34, 31, 31)), show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37944a092e7d3e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
