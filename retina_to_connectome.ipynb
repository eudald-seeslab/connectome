{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from graph_models import GNNModel\n",
    "from retina_to_connectome import get_activation_tensor, get_batch_voronoi_averages\n",
    "from adult_models_helpers import get_synapse_df\n",
    "from utils import flush_cuda_memory\n",
    "from flyvis.examples.flyvision_ans import DECODING_CELLS\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "last_good_frame = 8"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get data\n",
    "activations_dir = \"flyvis/parsed_objects\"\n",
    "activations = np.load(os.path.join(activations_dir, \"decoding_activations.npy\"), allow_pickle=True)\n",
    "classification = pd.read_csv(\"adult_data/classification.csv\")\n",
    "\n",
    "# remove duplicated root_ids\n",
    "classification = classification.drop_duplicates(subset='root_id')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1466ccb6d444b07",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "classification.head(20)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f083dab5af59a308",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "avgs_dict = {}\n",
    "for cell_type in tqdm(DECODING_CELLS):\n",
    "    number_of_cells = len(classification[classification[\"cell_type\"] == cell_type])\n",
    "    if number_of_cells > 0:\n",
    "        activation_tensor = get_activation_tensor(activations, cell_type, last_frame=last_good_frame) / 255\n",
    "        avgs_dict[cell_type] = get_batch_voronoi_averages(activation_tensor, n_centers=number_of_cells)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a91bc55a5c9604c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def voronoi_averages_to_df(dict_with_voronoi_averages):\n",
    "    dfs = []\n",
    "    for key, matrix in dict_with_voronoi_averages.items():\n",
    "        df = pd.DataFrame(matrix.transpose())\n",
    "        df['index_name'] = key\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all the DataFrames into one\n",
    "    return pd.concat(dfs, axis=0, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16ba36ae0792bc06",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "result_df = voronoi_averages_to_df(avgs_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66d6df94b20c7524",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extract cell types and activations\n",
    "cell_types = result_df.iloc[:, -1]  # Last column for cell type\n",
    "activations = result_df.iloc[:, :-1]  # Exclude the last column\n",
    "\n",
    "# Create a dictionary to hold shuffled root_ids for each cell type\n",
    "root_id_mapping = {}\n",
    "\n",
    "# Populate the dictionary with shuffled root_ids for each cell type\n",
    "for cell_type, group in classification.groupby(\"cell_type\"):\n",
    "    # Shuffle the root_ids within each group\n",
    "    shuffled_root_ids = group['root_id'].sample(frac=1).values\n",
    "    root_id_mapping[cell_type] = shuffled_root_ids\n",
    "\n",
    "# Function to assign root_ids to each row in result_df based on cell type and available root_ids\n",
    "def assign_root_ids(row):\n",
    "    cell_type = row.iloc[-1]  # Get cell type from the last column\n",
    "    # Get the list of shuffled root_ids for this cell type\n",
    "    root_ids = root_id_mapping[cell_type]\n",
    "    # Assign a root_id from the list, ensuring we don't exceed the list's length\n",
    "    # The index in the list is the count of occurrences of this cell type so far, modulo the number of available root_ids\n",
    "    root_id_index = row.name % len(root_ids)  # row.name is the index of the row in the dataframe\n",
    "    return root_ids[root_id_index]\n",
    "\n",
    "# Apply the function to result_df, creating a new 'root_id' column\n",
    "result_df['root_id'] = result_df.apply(assign_root_ids, axis=1)\n",
    "\n",
    "# Remove duplicated root_ids\n",
    "result_df = result_df.drop_duplicates(subset='root_id')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa33a3309a235441",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "activation_df = pd.merge(\n",
    "    classification.drop(\n",
    "        columns=[\"flow\", \"super_class\", \"class\", \"sub_class\", \n",
    "                 \"hemibrain_type\", \"hemilineage\", \"side\", \"nerve\"]), \n",
    "    result_df.drop(columns=[result_df.columns[-2]]), on='root_id', how='left').fillna(0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b9d37652bf3d75a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "synapse_df = get_synapse_df()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea5873de3ea67ca8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1: Identify Common Neurons\n",
    "# Unique root_ids in merged_df\n",
    "neurons_merged = pd.unique(activation_df['root_id'])\n",
    "\n",
    "# Unique root_ids in synapse_df (both pre and post)\n",
    "neurons_synapse_pre = pd.unique(synapse_df['pre_root_id'])\n",
    "neurons_synapse_post = pd.unique(synapse_df['post_root_id'])\n",
    "neurons_synapse = np.unique(np.concatenate([neurons_synapse_pre, neurons_synapse_post]))\n",
    "\n",
    "# Common neurons\n",
    "common_neurons = np.intersect1d(neurons_merged, neurons_synapse)\n",
    "\n",
    "# Step 2: Filter synapse_df\n",
    "# Keep only rows with both pre and post root_ids in common_neurons\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Filter synapse_df to include only rows with both pre and post root_ids in common_neurons\n",
    "filtered_synapse_df = synapse_df[\n",
    "    synapse_df['pre_root_id'].isin(common_neurons) & synapse_df['post_root_id'].isin(common_neurons)\n",
    "]\n",
    "\n",
    "# Map neuron root_ids to matrix indices\n",
    "root_id_to_index = {root_id: index for index, root_id in enumerate(common_neurons)}\n",
    "\n",
    "# Convert root_ids in filtered_synapse_df to matrix indices\n",
    "pre_indices = filtered_synapse_df['pre_root_id'].map(root_id_to_index).values\n",
    "post_indices = filtered_synapse_df['post_root_id'].map(root_id_to_index).values\n",
    "\n",
    "# Use syn_count as the data for the non-zero elements of the matrix\n",
    "data = filtered_synapse_df['syn_count'].values\n",
    "\n",
    "# Create a sparse matrix in COO format\n",
    "synaptic_matrix_sparse = coo_matrix(\n",
    "    (data, (pre_indices, post_indices)),\n",
    "    shape=(len(common_neurons), len(common_neurons)),\n",
    "    dtype=np.int64  # or np.float32/np.float64 if memory issue persists\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b9b0fc3eb3b1c75",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "activation_df = activation_df[activation_df['root_id'].isin(list(root_id_to_index.keys()))]\n",
    "activation_data = activation_df.drop(columns=[\"root_id\", \"cell_type\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac4670dc90dd0a2c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "from torch import autocast\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device_type = \"cpu\" # for debugging\n",
    "DEVICE = torch.device(device_type)\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "edges = torch.tensor([synaptic_matrix_sparse.row, synaptic_matrix_sparse.col], dtype=torch.long, device=DEVICE)\n",
    "activation_tensor = torch.tensor(activation_data.values, dtype=torch.float16, device=DEVICE)\n",
    "\n",
    "# Correctly set node features for each graph\n",
    "graph_list = []\n",
    "for i in range(activation_tensor.shape[1]):  # Iterate over samples, ensuring the second dimension is the sample dimension\n",
    "    node_features = activation_tensor[:, i].unsqueeze(1)  # Shape [num_nodes, 1], one feature per node\n",
    "    graph = Data(x=node_features.to(DEVICE), edge_index=edges)  # Create a graph for each sample\n",
    "    graph_list.append(graph)\n",
    "\n",
    "# DataLoader to handle batches of graphs\n",
    "loader = DataLoader(graph_list, batch_size=10, shuffle=False)\n",
    "\n",
    "# Random class labels for each graph\n",
    "# FIXME!!!! \n",
    "class_labels = torch.tensor(np.round(np.random.rand(100)).astype(\"int\"), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "# Initialize the model\n",
    "model = GNNModel(num_node_features=1, num_classes=2).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scaler = GradScaler()\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for batch_idx, batch in tqdm(enumerate(loader)):\n",
    "    batch = batch.to(DEVICE)\n",
    "\n",
    "    # Get the labels for the current batch\n",
    "    # Assuming your DataLoader does not automatically handle this\n",
    "    batch_labels = class_labels[batch_idx * batch_size : (batch_idx + 1) * batch_size].to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(device_type):\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, batch_labels)    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f726ab91ee1f4cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d77700314a2004a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO\n",
    "1. Literature review to identify the \"thinking\" neurons\n",
    "2. Identify these neurons in the classification dataframe and create a class_labels tensor\n",
    "3. Train the model with the class_labels tensor\n",
    "4. Check model accuracy and weber ratio\n",
    "5. Try with other model architectures, specially with a one-hot encoding for each neuron type to simulate different neurons"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acc97de0c76463cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
