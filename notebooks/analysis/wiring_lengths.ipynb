{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_276669/1475496359.py:6: DtypeWarning: Columns (14,15,16,17,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nc = pd.read_table(\n"
     ]
    }
   ],
   "source": [
    "connections = pd.read_csv(\n",
    "    \"../../new_data/connections.csv\",\n",
    "    dtype={\"pre_root_id\": \"string\", \"post_root_id\": \"string\", \"syn_count\": np.int32},\n",
    ")\n",
    "\n",
    "nc = pd.read_table(\n",
    "    \"../../new_data/neuron_annotations.tsv\",\n",
    "    dtype={\n",
    "        \"root_id\": \"string\",\n",
    "        \"soma_x\": np.float32,\n",
    "        \"soma_y\": np.float32,\n",
    "        \"soma_z\": np.float32,\n",
    "        \"cell_type\": \"string\",\n",
    "    },\n",
    ")[[\"root_id\", \"pos_x\", \"pos_y\", \"pos_z\", \"soma_x\", \"soma_y\", \"soma_z\", \"cell_type\"]]\n",
    "# fill missing soma nans with pos\n",
    "nc[\"soma_x\"] = nc[\"soma_x\"].fillna(nc[\"pos_x\"])\n",
    "nc[\"soma_y\"] = nc[\"soma_y\"].fillna(nc[\"pos_y\"])\n",
    "nc[\"soma_z\"] = nc[\"soma_z\"].fillna(nc[\"pos_z\"])\n",
    "nc = nc.drop(columns=[\"pos_x\", \"pos_y\", \"pos_z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_synapse_length(connections, nc):\n",
    "\n",
    "    df = connections.merge(\n",
    "        nc,\n",
    "        left_on=\"pre_root_id\",\n",
    "        right_on=\"root_id\",\n",
    "        suffixes=(\"\", \"_pre\"),\n",
    "    ).merge(\n",
    "        nc,\n",
    "        left_on=\"post_root_id\",\n",
    "        right_on=\"root_id\",\n",
    "        suffixes=(\"_pre\", \"_post\"),\n",
    "    )\n",
    "\n",
    "    # Drop unneeded columns to free memory\n",
    "    df = df.drop(columns=[\"root_id_pre\", \"root_id_post\", \"pre_root_id\", \"post_root_id\"])\n",
    "\n",
    "    # Vectorized distance calculation\n",
    "    distances = np.sqrt(\n",
    "        (df[\"soma_x_pre\"] - df[\"soma_x_post\"]) ** 2\n",
    "        + (df[\"soma_y_pre\"] - df[\"soma_y_post\"]) ** 2\n",
    "        + (df[\"soma_z_pre\"] - df[\"soma_z_post\"]) ** 2\n",
    "    )\n",
    "\n",
    "    # Multiply by synapse counts and sum\n",
    "    total_length = np.sum(distances * df[\"syn_count\"])\n",
    "\n",
    "    return total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = compute_total_synapse_length(connections, nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling strategy 1: shuffle randomly, then reduce synapse count until happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def shuffle_post_root_id(connections):\n",
    "    \"\"\"\n",
    "    Returns a copy of `connections` where the 'post_root_id' column\n",
    "    is randomly shuffled, leaving 'pre_root_id' and 'syn_count' intact.\n",
    "    \"\"\"\n",
    "    new_conns = connections.copy()\n",
    "    # Shuffle post_root_id in place\n",
    "    shuffled_posts = np.random.permutation(new_conns[\"post_root_id\"].values)\n",
    "    new_conns[\"post_root_id\"] = shuffled_posts\n",
    "    return new_conns\n",
    "\n",
    "\n",
    "def compute_total_synapse_length(connections, nc):\n",
    "    \"\"\"\n",
    "    Computes sum(distance * syn_count) by merging with `nc`.\n",
    "    connections: DataFrame with columns [pre_root_id, post_root_id, syn_count]\n",
    "    nc: DataFrame with columns [root_id, soma_x, soma_y, soma_z]\n",
    "    Returns a float (total wiring length).\n",
    "    \"\"\"\n",
    "    # Merge pre\n",
    "    df = connections.merge(\n",
    "        nc,\n",
    "        left_on=\"pre_root_id\",\n",
    "        right_on=\"root_id\",\n",
    "        suffixes=(\"\", \"_pre\"),\n",
    "    ).merge(\n",
    "        nc,\n",
    "        left_on=\"post_root_id\",\n",
    "        right_on=\"root_id\",\n",
    "        suffixes=(\"_pre\", \"_post\"),\n",
    "    )\n",
    "\n",
    "    # Drop unneeded columns\n",
    "    df = df.drop(columns=[\"root_id_pre\", \"root_id_post\", \"pre_root_id\", \"post_root_id\"])\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    dx = df[\"soma_x_pre\"] - df[\"soma_x_post\"]\n",
    "    dy = df[\"soma_y_pre\"] - df[\"soma_y_post\"]\n",
    "    dz = df[\"soma_z_pre\"] - df[\"soma_z_post\"]\n",
    "    dist = np.sqrt(dx * dx + dy * dy + dz * dz)\n",
    "\n",
    "    # Weighted sum\n",
    "    total_length = np.sum(dist * df[\"syn_count\"])\n",
    "    return total_length\n",
    "\n",
    "\n",
    "def match_wiring_length_with_syn_scale(\n",
    "    connections,\n",
    "    nc,\n",
    "    real_length,\n",
    "    scale_low=0.0,\n",
    "    scale_high=2.0,\n",
    "    max_iter=5,\n",
    "    tolerance=0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    Iteratively search for a global scale factor s in [scale_low, scale_high]\n",
    "    such that sum(distance * (syn_count*s)) ~ real_length.\n",
    "\n",
    "    connections: DataFrame [pre_root_id, post_root_id, syn_count]\n",
    "    nc         : DataFrame [root_id, soma_x, soma_y, soma_z]\n",
    "    real_length: Target total wiring length\n",
    "    scale_low  : initial lower bound on scale\n",
    "    scale_high : initial upper bound on scale\n",
    "    max_iter   : max number of binary search iterations\n",
    "    tolerance  : fraction (or absolute) to decide \"close enough\"\n",
    "\n",
    "    Returns the scaled DataFrame. The 'syn_count' column is overwritten\n",
    "    with the final scaled values.\n",
    "    \"\"\"\n",
    "    # Copy so we don't modify the original DataFrame\n",
    "    conns_scaled = connections.copy()\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        mid = 0.5 * (scale_low + scale_high)\n",
    "\n",
    "        # Temporarily scale syn_count by mid\n",
    "        conns_scaled[\"syn_count\"] = connections[\"syn_count\"] * mid\n",
    "        length_est = compute_total_synapse_length(conns_scaled, nc)\n",
    "\n",
    "        # Check how close we are\n",
    "        ratio = length_est / real_length\n",
    "        print(f\"Iter={i}, scale={mid:.4f}, length={length_est:.2f}, ratio={ratio:.3f}\")\n",
    "\n",
    "        # If we're still below the real length, we need a bigger scale\n",
    "        if length_est < real_length:\n",
    "            scale_low = mid\n",
    "        else:\n",
    "            scale_high = mid\n",
    "\n",
    "        # If ratio is within ~1 +/- tolerance, we can stop\n",
    "        if abs(ratio - 1.0) < tolerance:\n",
    "            break\n",
    "\n",
    "    # Final scale\n",
    "    final_scale = 0.5 * (scale_low + scale_high)\n",
    "    conns_scaled[\"syn_count\"] = connections[\"syn_count\"] * final_scale\n",
    "\n",
    "    return conns_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter=0, scale=1.0000, length=3231021020915.70, ratio=2.443\n",
      "Iter=1, scale=0.5000, length=1615510510457.85, ratio=1.222\n",
      "Iter=2, scale=0.2500, length=807755255228.93, ratio=0.611\n",
      "Iter=3, scale=0.3750, length=1211632882843.38, ratio=0.916\n",
      "Iter=4, scale=0.4375, length=1413571696650.62, ratio=1.069\n",
      "Iter=5, scale=0.4062, length=1312602289747.00, ratio=0.993\n",
      "Final scaled random total length = 1363086993198.8096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(12345)\n",
    "connections_shuffled = shuffle_post_root_id(connections)\n",
    "\n",
    "# 3) Use a small binary search to find a global syn_count scale factor\n",
    "#    that brings the total (dist * syn_count) close to the real_length\n",
    "scaled_random = match_wiring_length_with_syn_scale(\n",
    "    connections_shuffled,\n",
    "    nc,\n",
    "    total_length,\n",
    "    scale_low=0.0,\n",
    "    scale_high=2.0,\n",
    "    max_iter=10,\n",
    "    tolerance=0.01,\n",
    ")\n",
    "\n",
    "# 4) scaled_random now has syn_count scaled so that total wiring length\n",
    "#    is in the same ballpark as the real connectome\n",
    "final_length = compute_total_synapse_length(scaled_random, nc)\n",
    "print(\"Final scaled random total length =\", final_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_random[\"syn_count\"] = np.round(scaled_random[\"syn_count\"]).astype(np.int32)\n",
    "scaled_random.to_csv(\"../../new_data/connections_random.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling strategy 2: bin by distance, and shuffle within bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from paths import PROJECT_ROOT\n",
    "\n",
    "\n",
    "def shuffle_within_bin(group):\n",
    "    \"\"\"Shuffle entire rows within a bin\"\"\"\n",
    "    if len(group) <= 1:\n",
    "        return group\n",
    "\n",
    "    # Create a shuffled copy of the group\n",
    "    return group.sample(frac=1.0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_length_preserving_random_network(\n",
    "    connections, neurons, bins=10, tolerance=0.1\n",
    "):\n",
    "    print(\"Starting fully vectorized network randomization using pandas...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Ensure numeric types\n",
    "    connections = connections.copy()\n",
    "    neurons = neurons.copy()\n",
    "\n",
    "    connections[\"pre_root_id\"] = connections[\"pre_root_id\"].astype(int)\n",
    "    connections[\"post_root_id\"] = connections[\"post_root_id\"].astype(int)\n",
    "    connections[\"syn_count\"] = connections[\"syn_count\"].astype(int)\n",
    "    neurons[\"root_id\"] = neurons[\"root_id\"].astype(int)\n",
    "\n",
    "    # Identify retinal and decision neurons\n",
    "    retinal_ids = set(\n",
    "        neurons[neurons[\"cell_type\"].isin([\"R1-6\", \"R7\", \"R8\"])][\"root_id\"]\n",
    "    )\n",
    "    decision_ids = set(\n",
    "        neurons[neurons[\"cell_type\"].isin([\"KCapbp-m\", \"KCapbp-ap2\", \"KCapbp-ap1\"])][\n",
    "            \"root_id\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Identified {len(retinal_ids)} retinal neurons and {len(decision_ids)} decision neurons\"\n",
    "    )\n",
    "\n",
    "    # Create preserve mask\n",
    "    preserve_mask = connections[\"pre_root_id\"].isin(retinal_ids) | connections[\n",
    "        \"post_root_id\"\n",
    "    ].isin(decision_ids)\n",
    "\n",
    "    preserved_connections = connections[preserve_mask].copy()\n",
    "    randomizable_connections = connections[~preserve_mask].copy()\n",
    "\n",
    "    print(\n",
    "        f\"Preserved {len(preserved_connections)} connections, will randomize {len(randomizable_connections)}\"\n",
    "    )\n",
    "\n",
    "    # Join with neuron coordinates - single vectorized operation\n",
    "    # Add source neuron coordinates\n",
    "    pre_neurons = neurons[[\"root_id\", \"soma_x\", \"soma_y\", \"soma_z\"]].copy()\n",
    "    pre_neurons.columns = [\"pre_root_id\", \"pre_x\", \"pre_y\", \"pre_z\"]\n",
    "\n",
    "    # Add target neuron coordinates\n",
    "    post_neurons = neurons[[\"root_id\", \"soma_x\", \"soma_y\", \"soma_z\"]].copy()\n",
    "    post_neurons.columns = [\"post_root_id\", \"post_x\", \"post_y\", \"post_z\"]\n",
    "\n",
    "    # Join in one step\n",
    "    randomizable_with_coords = randomizable_connections.merge(\n",
    "        pre_neurons, on=\"pre_root_id\"\n",
    "    ).merge(post_neurons, on=\"post_root_id\")\n",
    "\n",
    "    # Calculate distances vectorized\n",
    "    print(\"Calculating distances (vectorized)...\")\n",
    "    randomizable_with_coords[\"distance\"] = np.sqrt(\n",
    "        (randomizable_with_coords[\"pre_x\"] - randomizable_with_coords[\"post_x\"]) ** 2\n",
    "        + (randomizable_with_coords[\"pre_y\"] - randomizable_with_coords[\"post_y\"]) ** 2\n",
    "        + (randomizable_with_coords[\"pre_z\"] - randomizable_with_coords[\"post_z\"]) ** 2\n",
    "    )\n",
    "\n",
    "    # Calculate original wiring length\n",
    "    original_total_length = (\n",
    "        randomizable_with_coords[\"distance\"] * randomizable_with_coords[\"syn_count\"]\n",
    "    ).sum()\n",
    "    print(\n",
    "        f\"Original total wiring length for randomizable connections: {original_total_length}\"\n",
    "    )\n",
    "\n",
    "    # Create distance bins\n",
    "    print(f\"Creating {bins} distance bins...\")\n",
    "    randomizable_with_coords[\"bin\"] = pd.qcut(\n",
    "        randomizable_with_coords[\"distance\"], bins, labels=False\n",
    "    )\n",
    "\n",
    "    # Shuffle within bins\n",
    "    print(\"Shuffling connections within distance bins (vectorized)...\")\n",
    "    shuffled_pre_ids = randomizable_with_coords[\"pre_root_id\"].copy()\n",
    "\n",
    "    # Replace the shuffling section with this:\n",
    "    print(\"Shuffling entire connections within distance bins...\")\n",
    "    shuffled_randomizable = pd.DataFrame()\n",
    "\n",
    "    # Group by bin and shuffle entire connections\n",
    "    shuffled_randomizable = pd.DataFrame()\n",
    "    for bin_id in tqdm(range(bins)):\n",
    "        bin_group = randomizable_with_coords[randomizable_with_coords[\"bin\"] == bin_id]\n",
    "        shuffled_bin = shuffle_within_bin(bin_group)\n",
    "        shuffled_randomizable = pd.concat(\n",
    "            [\n",
    "                shuffled_randomizable,\n",
    "                shuffled_bin[[\"pre_root_id\", \"post_root_id\", \"syn_count\"]],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Calculate distance for shuffled connections\n",
    "    shuffled_with_coords = shuffled_randomizable.merge(\n",
    "        pre_neurons, on=\"pre_root_id\"\n",
    "    ).merge(post_neurons, on=\"post_root_id\")\n",
    "\n",
    "    shuffled_with_coords[\"distance\"] = np.sqrt(\n",
    "        (shuffled_with_coords[\"pre_x\"] - shuffled_with_coords[\"post_x\"]) ** 2\n",
    "        + (shuffled_with_coords[\"pre_y\"] - shuffled_with_coords[\"post_y\"]) ** 2\n",
    "        + (shuffled_with_coords[\"pre_z\"] - shuffled_with_coords[\"post_z\"]) ** 2\n",
    "    )\n",
    "\n",
    "    # Calculate shuffled wiring length\n",
    "    shuffled_total_length = (\n",
    "        shuffled_with_coords[\"distance\"] * shuffled_with_coords[\"syn_count\"]\n",
    "    ).sum()\n",
    "\n",
    "    # Calculate preserved connections wiring length\n",
    "    preserved_with_coords = preserved_connections.merge(\n",
    "        pre_neurons, on=\"pre_root_id\"\n",
    "    ).merge(post_neurons, on=\"post_root_id\")\n",
    "\n",
    "    preserved_with_coords[\"distance\"] = np.sqrt(\n",
    "        (preserved_with_coords[\"pre_x\"] - preserved_with_coords[\"post_x\"]) ** 2\n",
    "        + (preserved_with_coords[\"pre_y\"] - preserved_with_coords[\"post_y\"]) ** 2\n",
    "        + (preserved_with_coords[\"pre_z\"] - preserved_with_coords[\"post_z\"]) ** 2\n",
    "    )\n",
    "\n",
    "    preserved_total_length = (\n",
    "        preserved_with_coords[\"distance\"] * preserved_with_coords[\"syn_count\"]\n",
    "    ).sum()\n",
    "\n",
    "    # Combine preserved and shuffled\n",
    "    final_connections = pd.concat(\n",
    "        [\n",
    "            preserved_connections[[\"pre_root_id\", \"post_root_id\", \"syn_count\"]],\n",
    "            shuffled_randomizable,\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    # Calculate total length\n",
    "    total_length = shuffled_total_length + preserved_total_length\n",
    "    print(f\"Shuffled randomizable wiring length: {shuffled_total_length}\")\n",
    "    print(f\"Preserved wiring length: {preserved_total_length}\")\n",
    "    print(f\"Total wiring length: {total_length}\")\n",
    "\n",
    "    # Calculate ratio\n",
    "    ratio = shuffled_total_length / original_total_length\n",
    "    print(f\"Ratio of randomized wiring lengths: {ratio:.4f}\")\n",
    "    print(f\"Target range: {1-tolerance:.4f} to {1+tolerance:.4f}\")\n",
    "\n",
    "    if not (1 - tolerance <= ratio <= 1 + tolerance):\n",
    "        print(\n",
    "            f\"Warning: Ratio {ratio:.4f} is outside the target range. You may want to try again with more bins.\"\n",
    "        )\n",
    "\n",
    "    print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    return final_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fully vectorized network randomization using pandas...\n",
      "Identified 11119 retinal neurons and 916 decision neurons\n",
      "Preserved 140514 connections, will randomize 16707483\n",
      "Calculating distances (vectorized)...\n",
      "Original total wiring length for randomizable connections: 1396282370978.2185\n",
      "Creating 20 distance bins...\n",
      "Shuffling connections within distance bins (vectorized)...\n",
      "Shuffling entire connections within distance bins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled randomizable wiring length: 1396282370978.2188\n",
      "Preserved wiring length: 4911623927.495159\n",
      "Total wiring length: 1401193994905.7139\n",
      "Ratio of randomized wiring lengths: 1.0000\n",
      "Target range: 0.9000 to 1.1000\n",
      "Total time: 18.60 seconds\n"
     ]
    }
   ],
   "source": [
    "random_connections = create_length_preserving_random_network(\n",
    "    connections, nc, bins=20, tolerance=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_connections.to_csv(os.path.join(PROJECT_ROOT, \"new_data\", \"connections_random3.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
