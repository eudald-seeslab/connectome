{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:40:02.816615Z",
     "start_time": "2024-03-28T14:39:58.709963Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "/home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch_geometric/typing.py:72: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch_geometric/typing.py:110: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/eudald/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import device, cuda, autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from random import sample\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "import flyvision\n",
    "from flyvision_ans import DECODING_CELLS\n",
    "from flyvision.utils.activity_utils import LayerActivity\n",
    "from from_image_to_video import image_paths_to_sequences\n",
    "from from_retina_to_connectome_funcs import from_retina_to_model, get_cell_type_indices, compute_voronoi_averages, from_retina_to_connectome\n",
    "from logs_to_wandb import log_images_to_wandb, log_running_stats_to_wandb\n",
    "from from_video_to_training_batched_funcs import get_files_from_directory, select_random_videos, paths_to_labels, \\\n",
    "    load_custom_sequences\n",
    "from from_retina_to_connectome_utils import (\n",
    "    hex_to_square_grid,\n",
    "    initialize_results_df,\n",
    "    predictions_and_corrects_from_model_results,\n",
    "    update_results_df,\n",
    "    update_running_loss,\n",
    "    get_decision_making_neurons,\n",
    "    vector_to_one_hot,\n",
    ")\n",
    "from adult_models import FullAdultModel\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='invalid value encountered in cast',\n",
    "    category=RuntimeWarning,\n",
    "    module='wandb.sdk.data_types.image'\n",
    ")\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "dtype = torch.float32\n",
    "\n",
    "device_type = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "device_type = \"cpu\"\n",
    "DEVICE = device(device_type)\n",
    "\n",
    "TRAINING_DATA_DIR = \"images/easy_v2\"\n",
    "TESTING_DATA_DIR = \"images/easy_images\"\n",
    "VALIDATION_DATA_DIR = \"images/easyval_images\"\n",
    "\n",
    "debugging = True\n",
    "debug_length = 20\n",
    "wandb_ = False\n",
    "wandb_images_every = 100\n",
    "small = True\n",
    "small_length = 2000\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "\n",
    "dropout = .1\n",
    "max_lr = 0.01\n",
    "base_lr = 0.001\n",
    "weight_decay = 0.0001\n",
    "NUM_CONNECTOME_PASSES=1\n",
    "\n",
    "use_one_cycle_lr = False\n",
    "\n",
    "model_config = {\n",
    "    \"debugging\": debugging,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"dropout\": dropout,\n",
    "    \"base_lr\": base_lr,\n",
    "    \"max_lr\": max_lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"num_connectome_passes\": NUM_CONNECTOME_PASSES,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b101e44d143f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:18:59.226154Z",
     "start_time": "2024-03-28T14:18:54.651619Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init stuff\n",
    "extent, kernel_size = 15, 13\n",
    "decision_making_vector = get_decision_making_neurons(dtype)\n",
    "receptors = flyvision.rendering.BoxEye(extent=extent, kernel_size=kernel_size)\n",
    "network_view = flyvision.NetworkView(flyvision.results_dir / \"opticflow/000/0000\")\n",
    "network = network_view.init_network(chkpt=\"best_chkpt\")\n",
    "classification = pd.read_csv(\"adult_data/classification_clean.csv\")\n",
    "root_id_to_index = pd.read_csv(\"adult_data/root_id_to_index.csv\")\n",
    "dt = 1 / 100 # some parameter from flyvision\n",
    "last_good_frame = 2\n",
    "cell_type_plot = \"TmY18\"\n",
    "\n",
    "cell_type_indices = get_cell_type_indices(classification, root_id_to_index, DECODING_CELLS)\n",
    "\n",
    "training_videos = get_files_from_directory(TRAINING_DATA_DIR)\n",
    "test_videos = get_files_from_directory(TESTING_DATA_DIR)\n",
    "validation_videos = get_files_from_directory(TESTING_DATA_DIR)\n",
    "\n",
    "if small:\n",
    "    training_videos = sample(training_videos, small_length)\n",
    "    test_videos = sample(test_videos, small_length)\n",
    "    validation_videos = sample(validation_videos, int(small_length / 5))\n",
    "\n",
    "if len(training_videos) == 0:\n",
    "    print(\"I can't find any training images or videos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fce80a3502cf3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:18:59.238988Z",
     "start_time": "2024-03-28T14:18:59.227248Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sparse params at indices [[0, 0]]: SparseAdam requires dense parameter tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m one_hot_decision_making \u001b[38;5;241m=\u001b[39m vector_to_one_hot(decision_making_vector, dtype)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m FullAdultModel(\n\u001b[1;32m      5\u001b[0m     synaptic_matrix,\n\u001b[1;32m      6\u001b[0m     one_hot_decision_making,\n\u001b[1;32m      7\u001b[0m     NUM_CONNECTOME_PASSES,\n\u001b[1;32m      8\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m      9\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparseAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m scaler \u001b[38;5;241m=\u001b[39m GradScaler()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the loss function\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/optim/sparse_adam.py:29\u001b[0m, in \u001b[0;36mSparseAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, maximize)\u001b[0m\n\u001b[1;32m     27\u001b[0m             sparse_params\u001b[38;5;241m.\u001b[39mappend([index, d_index])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_params:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparse params at indices \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: SparseAdam requires dense parameter tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Sparse params at indices [[0, 0]]: SparseAdam requires dense parameter tensors"
     ]
    }
   ],
   "source": [
    "synaptic_matrix = load_npz(\"adult_data/synaptic_matrix_sparse.npz\")\n",
    "one_hot_decision_making = vector_to_one_hot(decision_making_vector, dtype).to(DEVICE)\n",
    "\n",
    "model = FullAdultModel(\n",
    "    synaptic_matrix,\n",
    "    one_hot_decision_making,\n",
    "    NUM_CONNECTOME_PASSES,\n",
    "    dtype=dtype,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=base_lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa2f7661646a93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:22:54.362080Z",
     "start_time": "2024-03-28T14:19:58.045941Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/home/eudald/Desktop/doctorat/connectome/adult_models.py:34: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  x = torch.sparse.mm(self.shared_weights, x) #+ self.shared_bias\n",
      "  0%|          | 0/20 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Adam does not support sparse gradients, please consider SparseAdam instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# scaler.scale(loss).backward()\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# scaler.step(optimizer)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# scaler.update()\u001b[39;00m\n\u001b[1;32m     69\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 70\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Calculate run parameters\u001b[39;00m\n\u001b[1;32m     73\u001b[0m predictions, labels_cpu, correct \u001b[38;5;241m=\u001b[39m predictions_and_corrects_from_model_results(out, labels)\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/venv/lib/python3.10/site-packages/torch/optim/adam.py:96\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     94\u001b[0m params_with_grad\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam does not support sparse gradients, please consider SparseAdam instead\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     97\u001b[0m grads\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m     99\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[p]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Adam does not support sparse gradients, please consider SparseAdam instead"
     ]
    }
   ],
   "source": [
    "if wandb_:\n",
    "    wandb.init(project=\"adult_connectome\", config=model_config)\n",
    "\n",
    "model.train()\n",
    "\n",
    "results = initialize_results_df()\n",
    "probabilities = []\n",
    "accuracies = []\n",
    "already_selected = []\n",
    "running_loss = 0.0\n",
    "total_correct = 0\n",
    "total = 0\n",
    "\n",
    "iterations = debug_length if debugging else len(training_videos) // batch_size\n",
    "\n",
    "for i in tqdm(range(iterations)):\n",
    "    batch_files, already_selected = select_random_videos(\n",
    "        training_videos, batch_size, already_selected\n",
    "    )\n",
    "    labels = paths_to_labels(batch_files)\n",
    "    batch_sequences = image_paths_to_sequences(batch_files)\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "\n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        # rendered sequences are in RGB; move it to 0-1 for better training\n",
    "        rendered_sequence = torch.div(rendered_sequence, 255)\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(\n",
    "            LayerActivity(simulation, network.connectome, keepref=True)\n",
    "        )\n",
    "\n",
    "    if wandb_ and i % wandb_images_every == 0:\n",
    "        la_0 = hex_to_square_grid(\n",
    "            layer_activations[0][cell_type_plot].squeeze()[-last_good_frame].cpu().numpy()\n",
    "            ),\n",
    "        log_images_to_wandb(batch_sequences[0], rendered_sequences[0], la_0, batch_files[0], frame=last_good_frame, cell_type=cell_type_plot)\n",
    "\n",
    "    voronoi_averages_df = compute_voronoi_averages(\n",
    "        layer_activations, classification, DECODING_CELLS, last_good_frame=last_good_frame\n",
    "    )\n",
    "    # normalize column wise (except last column) using apply\n",
    "    values_cols = voronoi_averages_df.columns != \"index_name\"\n",
    "    voronoi_averages_df.loc[:, values_cols] = voronoi_averages_df.loc[:, values_cols].apply(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()), axis=0\n",
    "        )\n",
    "\n",
    "    activation_df = from_retina_to_connectome(\n",
    "        voronoi_averages_df, classification, root_id_to_index\n",
    "    )\n",
    "    del layer_activations, rendered_sequences, rendered_sequence, simulation\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if wandb_:\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=20)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # inputs = create_csr_input(activation_df)\n",
    "    inputs = torch.tensor(activation_df.values, dtype=dtype, device=DEVICE).to_sparse()\n",
    "    labels = torch.tensor(labels, dtype=dtype, device=DEVICE)\n",
    "\n",
    "    out = model(inputs)\n",
    "    loss = criterion(out, labels)\n",
    "    # scaler.scale(loss).backward()\n",
    "    # scaler.step(optimizer)\n",
    "    # scaler.update()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate run parameters\n",
    "    predictions, labels_cpu, correct = predictions_and_corrects_from_model_results(out, labels)\n",
    "    results = update_results_df(results, batch_files, predictions, labels_cpu, correct)\n",
    "    running_loss += update_running_loss(loss, inputs)\n",
    "    total += labels.shape[0]\n",
    "    total_correct += correct.sum()\n",
    "\n",
    "    if wandb_:\n",
    "        log_running_stats_to_wandb(0, i, running_loss, total_correct, total, results)\n",
    "\n",
    "print(f\"Finished training with loss {running_loss / total} and accuracy {total_correct / total}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33a081fe1b9a883e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T09:37:12.123162Z",
     "start_time": "2024-03-28T09:37:11.886793Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1960 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m batch_sequences \u001b[38;5;241m=\u001b[39m load_custom_sequences(batch_files)  \u001b[38;5;66;03m# Load and preprocess the video sequences\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming receptors is a function that processes your sequences\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m rendered_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mreceptors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m layer_activations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rendered_sequence \u001b[38;5;129;01min\u001b[39;00m rendered_sequences:\n",
      "File \u001b[0;32m~/Desktop/doctorat/connectome/flyvision/rendering/eye.py:116\u001b[0m, in \u001b[0;36mBoxEye.__call__\u001b[0;34m(self, sequence, ftype, hex_sample)\u001b[0m\n\u001b[1;32m    111\u001b[0m samples, frames, height, width \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sequence, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mFloatTensor):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# auto-moving to GPU in case default tensor is cuda but passed\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# sequence is not for convenience\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_frame_size \u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([height, width]))\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# to rescale to the minimum frame size\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m ttf\u001b[38;5;241m.\u001b[39mresize(\n\u001b[1;32m    121\u001b[0m         sequence, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_frame_size\u001b[38;5;241m.\u001b[39mtolist(), antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "already_selected_validation = []\n",
    "# Assuming batch_size is defined\n",
    "for _ in tqdm(range(len(validation_videos) // batch_size)):\n",
    "    batch_files, already_selected_validation = select_random_videos(validation_videos, batch_size, already_selected_validation)\n",
    "\n",
    "    labels = paths_to_labels(batch_files)  # Convert paths to labels\n",
    "    batch_sequences = load_custom_sequences(batch_files)  # Load and preprocess the video sequences\n",
    "    \n",
    "    # Assuming receptors is a function that processes your sequences\n",
    "    rendered_sequences = receptors(batch_sequences)\n",
    "    \n",
    "    layer_activations = []\n",
    "    for rendered_sequence in rendered_sequences:\n",
    "        simulation = network.simulate(rendered_sequence[None], dt)\n",
    "        layer_activations.append(LayerActivity(simulation, network.connectome, keepref=True))\n",
    "        \n",
    "    del rendered_sequences, simulation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Preparing the data for the model, similar to training\n",
    "    inputs, labels = from_retina_to_model(layer_activations, labels, DECODING_CELLS, last_good_frame, classification, root_id_to_index)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).unsqueeze(-1).float()\n",
    "        \n",
    "        with autocast(device_type):\n",
    "            predictions = model(inputs)\n",
    "            # Assuming your criterion and evaluation metrics are defined similarly to training\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss.append(loss.item())\n",
    "            # Calculate other metrics if necessary, e.g., accuracy\n",
    "            \n",
    "            # Log validation metrics to WandB\n",
    "            wandb.log({\"validation_loss\": loss.item()})\n",
    "            # Log other metrics similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e93414a83f7bbd3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([134191, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dabc4d2827d8cc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When cuda breaks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba39b3687e323bec",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for eudald: \n",
      "[sudo] password for eudald: \n"
     ]
    }
   ],
   "source": [
    "!sudo rmmod nvidia_uvm\n",
    "!sudo modprobe nvidia_uvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0185e185",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "-------------------------------------------------------------------------\n",
      "        Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "   AdultConnectome-1         [134191, 2]  18,007,224,481  18,007,224,481\n",
      "            Linear-2         [2, 134191]         134,192         134,192\n",
      "=========================================================================\n",
      "Total params: 18,007,358,673\n",
      "Trainable params: 18,007,358,673\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pytorch_model_summary import summary\n",
    "\n",
    "print(summary(model, torch.zeros(inputs.shape), show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8af231b6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero entries (trainable parameters): 3871467\n",
      "Shape of the sparse tensor: torch.Size([134191, 134191])\n",
      "Number of parameters (non-zero entries): 3871467\n"
     ]
    }
   ],
   "source": [
    "# Correctly print number of non-zero entries\n",
    "print(\"Number of non-zero entries (trainable parameters):\", model.connectome.shared_weights._values().numel())\n",
    "print(\"Shape of the sparse tensor:\", model.connectome.shared_weights.shape)\n",
    "print(\"Number of parameters (non-zero entries):\", model.connectome.shared_weights._nnz())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eeef86c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([134191, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9088f10",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model.adul"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
