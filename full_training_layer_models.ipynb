{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:40:02.816615Z",
     "start_time": "2024-03-28T14:39:58.709963Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import device, cuda\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "from random import sample\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "import flyvision\n",
    "from flyvision_ans import DECODING_CELLS, FINAL_CELLS\n",
    "from from_retina_to_connectome_funcs import get_cell_type_indices\n",
    "from from_video_to_training_batched_funcs import get_files_from_directory, select_random_videos, paths_to_labels\n",
    "from from_retina_to_connectome_utils import (\n",
    "    initialize_results_df,\n",
    "    predictions_and_corrects_from_model_results,\n",
    "    update_results_df,\n",
    "    update_running_loss,\n",
    "    get_decision_making_neurons,\n",
    "    vector_to_one_hot,\n",
    ")\n",
    "from adult_models import FullAdultModel\n",
    "from wandb_utils import WandBLogger\n",
    "from full_training_data_processing import FullModelsDataProcessor\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='invalid value encountered in cast',\n",
    "    category=RuntimeWarning,\n",
    "    module='wandb.sdk.data_types.image'\n",
    ")\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "dtype = torch.float32\n",
    "\n",
    "device_type = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "# device_type = \"cpu\"\n",
    "DEVICE = device(device_type)\n",
    "sparse_layout = torch.sparse_coo\n",
    "\n",
    "TRAINING_DATA_DIR = \"images/easy_v2\"\n",
    "TESTING_DATA_DIR = \"images/easy_images\"\n",
    "VALIDATION_DATA_DIR = \"images/easyval_images\"\n",
    "\n",
    "debugging = False\n",
    "debug_length = 100\n",
    "validation_length = 50\n",
    "wandb_ = True\n",
    "wandb_images_every = 100\n",
    "small = True\n",
    "small_length = 400\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "\n",
    "dropout = .1\n",
    "max_lr = 0.01\n",
    "base_lr = 0.001\n",
    "weight_decay = 0.0001\n",
    "NUM_CONNECTOME_PASSES = 10\n",
    "final_retina_cells = FINAL_CELLS\n",
    "normalize_voronoi_cells = True\n",
    "\n",
    "model_config = {\n",
    "    \"debugging\": debugging,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"dropout\": dropout,\n",
    "    \"base_lr\": base_lr,\n",
    "    \"max_lr\": max_lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"num_connectome_passes\": NUM_CONNECTOME_PASSES,\n",
    "}\n",
    "wandb_logger = WandBLogger(\n",
    "    \"adult_connectome\",\n",
    "    model_config,\n",
    "    enabled=wandb_,\n",
    "    log_images_every=wandb_images_every,\n",
    "    cell_type_plot=\"TmY18\",\n",
    "    last_good_frame=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b101e44d143f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:18:59.226154Z",
     "start_time": "2024-03-28T14:18:54.651619Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init data stuff\n",
    "# TODO: move this to the data processing script\n",
    "extent, kernel_size = 15, 13\n",
    "decision_making_vector = get_decision_making_neurons(dtype)\n",
    "receptors = flyvision.rendering.BoxEye(extent=extent, kernel_size=kernel_size)\n",
    "network_view = flyvision.NetworkView(flyvision.results_dir / \"opticflow/000/0000\")\n",
    "network = network_view.init_network(chkpt=\"best_chkpt\")\n",
    "classification = pd.read_csv(\"adult_data/classification_clean.csv\")\n",
    "root_id_to_index = pd.read_csv(\"adult_data/root_id_to_index.csv\")\n",
    "cell_type_plot = \"TmY18\"\n",
    "\n",
    "cell_type_indices = get_cell_type_indices(\n",
    "    classification, root_id_to_index, final_retina_cells\n",
    ")\n",
    "\n",
    "training_videos = get_files_from_directory(TRAINING_DATA_DIR)\n",
    "test_videos = get_files_from_directory(TESTING_DATA_DIR)\n",
    "validation_videos = get_files_from_directory(TESTING_DATA_DIR)\n",
    "\n",
    "if small:\n",
    "    training_videos = sample(training_videos, small_length)\n",
    "    test_videos = sample(test_videos, small_length)\n",
    "    validation_videos = sample(validation_videos, int(small_length / 5))\n",
    "\n",
    "if len(training_videos) == 0:\n",
    "    print(\"I can't find any training images or videos!\")\n",
    "\n",
    "synaptic_matrix = load_npz(\"adult_data/synaptic_matrix_sparse.npz\")\n",
    "one_hot_decision_making = vector_to_one_hot(\n",
    "    decision_making_vector, dtype, sparse_layout\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce80a3502cf3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T14:18:59.238988Z",
     "start_time": "2024-03-28T14:18:59.227248Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init models and data processor\n",
    "model = FullAdultModel(\n",
    "    synaptic_matrix,\n",
    "    one_hot_decision_making,\n",
    "    cell_type_indices,\n",
    "    NUM_CONNECTOME_PASSES,\n",
    "    log_transform_weights=True,\n",
    "    sparse_layout=sparse_layout,\n",
    "    dtype=dtype,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "wandb_logger.initialize()\n",
    "# initialize the data processor\n",
    "data_processor = FullModelsDataProcessor(\n",
    "    wandb_logger=wandb_logger,\n",
    "    receptors=receptors,\n",
    "    network=network,\n",
    "    classification=classification,\n",
    "    final_retina_cells=final_retina_cells,\n",
    "    normalize_voronoi_cells=normalize_voronoi_cells,\n",
    "    root_id_to_index=root_id_to_index,\n",
    "    dtype=dtype,\n",
    "    DEVICE=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "results = initialize_results_df()\n",
    "already_selected = []\n",
    "running_loss, total_correct, total = 0, 0, 0\n",
    "\n",
    "model.train()\n",
    "iterations = debug_length if debugging else len(training_videos) // batch_size\n",
    "for i in tqdm(range(iterations)):\n",
    "    batch_files, already_selected = select_random_videos(\n",
    "        training_videos, batch_size, already_selected\n",
    "    )\n",
    "    labels, inputs = data_processor.process_full_models_data(i, batch_files)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(inputs)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate run parameters\n",
    "    predictions, labels_cpu, correct = predictions_and_corrects_from_model_results(out, labels)\n",
    "    results = update_results_df(results, batch_files, predictions, labels_cpu, correct)\n",
    "    running_loss += update_running_loss(loss, inputs)\n",
    "    total += batch_size\n",
    "    total_correct += correct.sum()\n",
    "\n",
    "    wandb_logger.log_metrics(i, running_loss, total_correct, total, results)\n",
    "\n",
    "print(f\"Finished training with loss {running_loss / total} and accuracy {total_correct / total}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a081fe1b9a883e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T09:37:12.123162Z",
     "start_time": "2024-03-28T09:37:11.886793Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "already_selected_validation = []\n",
    "total_correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "validation_results = initialize_results_df()\n",
    "\n",
    "validation_iterations = validation_length if validation_length is not None else len(validation_videos) // batch_size\n",
    "for j in tqdm(range(validation_iterations)):\n",
    "    batch_files, already_selected_validation = select_random_videos(validation_videos, batch_size, already_selected_validation)\n",
    "\n",
    "    labels, inputs = data_processor.process_full_models_data(\n",
    "        wandb_logger, j, batch_files\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        predictions, batch_labels_cpu, correct = predictions_and_corrects_from_model_results(outputs, labels)\n",
    "        validation_results = update_results_df(validation_results, batch_files, predictions, batch_labels_cpu, correct)\n",
    "        running_loss += update_running_loss(loss, inputs)\n",
    "        total += batch_labels_cpu.shape[0]\n",
    "        total_correct += correct.sum().item()\n",
    "\n",
    "wandb_logger.log_metrics(0, running_loss, total_correct, total, validation_results)\n",
    "wandb_logger.finish()\n",
    "\n",
    "print(\n",
    "    f\"Validation Loss: {running_loss / total}, \"\n",
    "    f\"Validation Accuracy: {total_correct / total}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185e185",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_model_summary import summary\n",
    "input_shape = torch.Size([134191, 1])\n",
    "print(summary(model, torch.zeros(input_shape), show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d454f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "625866fb",
   "metadata": {},
   "source": [
    "# Coses a fer\n",
    "- petar-se el model de visió\n",
    "- comprovar que el backprop es fa bé\n",
    "- fer imatges amb menys punts i més grans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901ab56",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
